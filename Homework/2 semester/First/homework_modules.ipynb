{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook would be imported in the `homework_main-basic.ipynb` and `homework_main-advanced.ipynb` using `%run homework_modules.ipynb` command.\n",
    "So each cell of this notebook would be call.\n",
    "\n",
    "Thus, If you have some tests falling, just commit them out to check a functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TESTS ONLY!\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy\n",
    "import traceback\n",
    "\n",
    "VERBOSE = True\n",
    "\n",
    "def assertAlmostEqual(expected, actual, msg: str=\"???\", rtol=1e-05, atol=1e-08,):\n",
    "    try:\n",
    "        isEq = np.allclose(expected, actual, rtol=rtol, atol=atol)\n",
    "    except Exception as err:\n",
    "        print(f\"{msg}: FAILED:\\n  {err}\")\n",
    "        raise\n",
    "\n",
    "    if isEq:\n",
    "        if VERBOSE:\n",
    "            print(f\"{msg}: OK!\")\n",
    "    else:\n",
    "        print(f\"{msg}: FAILED:\\n  expected={expected}\\n  actual={actual}\")\n",
    "        raise AssertionError(f\"{msg}: FAILED:\\n  expected={expected}\\  actual={actual}\")\n",
    "        \n",
    "def assertTrue(condition, msg: str=\"???\"):\n",
    "    if condition:\n",
    "        if VERBOSE:\n",
    "            print(f\"{msg}: OK!\")\n",
    "    else:\n",
    "        print(f\"{msg}: FAILED:\\n  expected=True\")\n",
    "        raise AssertionError(f\"{msg}: FAILED:\\n  expected=True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Module** is an abstract class which defines fundamental methods necessary for a training a neural network. You do not need to change anything here, just read the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\"\n",
    "    Basically, you can think of a module as of a something (black box) \n",
    "    which can process `input` data and produce `ouput` data.\n",
    "    This is like applying a function which is called `forward`: \n",
    "        \n",
    "        output = module.forward(input)\n",
    "    \n",
    "    The module should be able to perform a backward pass: to differentiate the `forward` function. \n",
    "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
    "    The latter implies there is a gradient from previous step of a chain rule. \n",
    "    \n",
    "        gradInput = module.backward(input, gradOutput)\n",
    "    \"\"\"\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        self.training = True\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes an input object, and computes the corresponding output of the module.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input)\n",
    "\n",
    "    def backward(self,input, gradOutput):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the module, with respect to the given input.\n",
    "        \n",
    "        This includes \n",
    "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
    "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
    "        \"\"\"\n",
    "        self.updateGradInput(input, gradOutput)\n",
    "        self.accGradParameters(input, gradOutput)\n",
    "        return self.gradInput\n",
    "    \n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"\n",
    "        Computes the output using the current parameter set of the class and input.\n",
    "        This function returns the result which is stored in the `output` field.\n",
    "        \n",
    "        Make sure to both store the data in `output` field and return it. \n",
    "        \"\"\"\n",
    "        \n",
    "        # The easiest case:\n",
    "            \n",
    "        # self.output = input \n",
    "        # return self.output\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own input. \n",
    "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
    "        \n",
    "        The shape of `gradInput` is always the same as the shape of `input`.\n",
    "        \n",
    "        Make sure to both store the gradients in `gradInput` field and return it.\n",
    "        \"\"\"\n",
    "        \n",
    "        # The easiest case:\n",
    "        \n",
    "        # self.gradInput = gradOutput \n",
    "        # return self.gradInput\n",
    "        \n",
    "        pass   \n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own parameters.\n",
    "        No need to override if module has no parameters (e.g. ReLU).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self): \n",
    "        \"\"\"\n",
    "        Zeroes `gradParams` variable if the module has params.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with its parameters. \n",
    "        If the module does not have parameters return empty list. \n",
    "        \"\"\"\n",
    "        return []\n",
    "        \n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with gradients with respect to its parameters. \n",
    "        If the module does not have parameters return empty list. \n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Sets training mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Sets evaluation mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want \n",
    "        to have readable description. \n",
    "        \"\"\"\n",
    "        return \"Module\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define** a forward and backward pass procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "         This class implements a container, which processes `input` data sequentially. \n",
    "         \n",
    "         `input` is processed by each module (layer) in self.modules consecutively.\n",
    "         The resulting array is called `output`. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "   \n",
    "    def add(self, module):\n",
    "        \"\"\"\n",
    "        Adds a module to the container.\n",
    "        \"\"\"\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"\n",
    "        Basic workflow of FORWARD PASS:\n",
    "        \n",
    "            y_0    = module[0].forward(input)\n",
    "            y_1    = module[1].forward(y_0)\n",
    "            ...\n",
    "            output = module[n-1].forward(y_{n-2})   \n",
    "            \n",
    "            \n",
    "        Just write a little loop. \n",
    "        \"\"\"\n",
    "        module = self.modules[0].forward(input)\n",
    "        for i in range(1,len(self.modules)):\n",
    "            module = self.modules[i].forward(module)\n",
    "        self.output = module\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Workflow of BACKWARD PASS:\n",
    "            \n",
    "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
    "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
    "            ...\n",
    "            g_1 = module[1].backward(y_0, g_2)   \n",
    "            gradInput = module[0].backward(input, g_1)   \n",
    "             \n",
    "             \n",
    "        !!!\n",
    "                \n",
    "        To ech module you need to provide the input, module saw while forward pass, \n",
    "        it is used while computing gradients. \n",
    "        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass) \n",
    "        and NOT `input` to this Sequential module. \n",
    "        \n",
    "        !!!\n",
    "        \n",
    "        \"\"\"\n",
    "        # Your code goes here. ################################################\n",
    "        n = len(self.modules)\n",
    "        g = self.modules[n-1].backward(y[n-2],gradOutput)\n",
    "        for i in range(n-2,-1,-1):\n",
    "            g = self.modules[i].backward(self.modules[i-1].output,g)\n",
    "        g_first = self.modules[1].backward(self.modules[0].output,g)\n",
    "        self.gradInput = self.modules[0].backward(input,g_first)\n",
    "        return self.gradInput\n",
    "      \n",
    "\n",
    "    def zeroGradParameters(self): \n",
    "        for module in self.modules:\n",
    "            module.zeroGradParameters()\n",
    "    \n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.getParameters() for x in self.modules]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all gradients w.r.t parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.getGradParameters() for x in self.modules]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
    "        return string\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Propagates training parameter through all modules\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "        for module in self.modules:\n",
    "            module.train()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Propagates training parameter through all modules\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "        for module in self.modules:\n",
    "            module.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BatchNormalization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6fad8a207a47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0massertAlmostEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_bias_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'3. check layer parameters grad. bias'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mtest_Sequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-6fad8a207a47>\u001b[0m in \u001b[0;36mtest_Sequential\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# layers initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtorch_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtorch_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mcustom_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BatchNormalization' is not defined"
     ]
    }
   ],
   "source": [
    "def test_Sequential():\n",
    "    # Unfortunately this test you cannot run right now. \n",
    "    # It would be best to run it after implementing Linear layer and test it with Linear\n",
    "    # instead of Batch norm\n",
    "    \n",
    "    # TODO: as a student you can try to fix it, or not. \n",
    "\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    batch_size, n_in = 2, 4\n",
    "    for i in range(100):\n",
    "        print(f\"Iter {i}\")\n",
    "        # layers initialization\n",
    "        alpha = 0.9\n",
    "        torch_layer = torch.nn.BatchNorm1d(n_in, eps=BatchNormalization.EPS, momentum=1.-alpha, affine=True)\n",
    "        torch_layer.bias.data = torch.from_numpy(np.random.random(n_in).astype(np.float32))\n",
    "        custom_layer = Sequential()\n",
    "        bn_layer = BatchNormalization(alpha)\n",
    "        bn_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n",
    "        bn_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
    "        custom_layer.add(bn_layer)\n",
    "        scaling_layer = ChannelwiseScaling(n_in)\n",
    "        scaling_layer.gamma = torch_layer.weight.data.numpy()\n",
    "        scaling_layer.beta = torch_layer.bias.data.numpy()\n",
    "        custom_layer.add(scaling_layer)\n",
    "        custom_layer.train()\n",
    "\n",
    "        layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "        next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "        # 1. check layer output\n",
    "        custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "        torch_layer_output_var = torch_layer(layer_input_var)\n",
    "        assertAlmostEqual(torch_layer_output_var.data.numpy(), custom_layer_output, msg='1. check layer output')\n",
    "\n",
    "        # 2. check layer input grad\n",
    "        custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n",
    "        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "        torch_layer_grad_var = layer_input_var.grad\n",
    "        assertAlmostEqual(torch_layer_grad_var.data.numpy(), custom_layer_grad, msg='2. check layer input grad')\n",
    "\n",
    "        # 3. check layer parameters grad\n",
    "        weight_grad, bias_grad = custom_layer.getGradParameters()[1]\n",
    "        torch_weight_grad = torch_layer.weight.grad.data.numpy()\n",
    "        torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
    "        assertAlmostEqual(torch_weight_grad, weight_grad, msg='3. check layer parameters grad. weights')\n",
    "        assertAlmostEqual(torch_bias_grad, bias_grad, msg='3. check layer parameters grad. bias')\n",
    "        \n",
    "test_Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear transform layer\n",
    "Also known as dense layer, fully-connected layer, FC-layer, InnerProductLayer (in caffe), affine transform\n",
    "- input:   **`batch_size x n_feats1`**\n",
    "- output: **`batch_size x n_feats2`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    A module which applies a linear transformation \n",
    "    A common name is fully-connected layer, InnerProductLayer in caffe. \n",
    "    \n",
    "    The module should work with 2D input of shape (n_samples, n_feature).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "        self.learning_rate = 0.1\n",
    "        # This is a nice initialization\n",
    "        stdv = 1./np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size = (n_out,n_in)) \n",
    "        print(\"W shape is {0} in init1\\n\".format(self.W.shape))\n",
    "        #was n_out,n_in, change to n_in,n_out -> doesn't change shape for np.dot. WTF?\n",
    "        self.b = np.random.uniform(-stdv, stdv, size = (n_out, 1))\n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        print(\"W shape is {0} in init2\\n\".format(self.W.shape))\n",
    "        print(\"Wgrad shape is {0} in init\\n\".format(self.gradW.shape))\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        print(\"W shape is {0} in updateOutput\\n\".format(self.W.shape))\n",
    "        self.output = np.dot(input, self.W.T)\n",
    "        self.output += self.b\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        print(\"W shape is {0} in updateGrad\\n\".format(self.W.shape))\n",
    "        print(\"Wgrad shape is {0} in upgradeGrad1\\n\".format(self.gradW.shape))\n",
    "        self.gradInput = gradOutput @ self.W\n",
    "        print(\"Wgrad shape is {0} in upgradeGrad2\\n\".format(self.gradW.shape))\n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        print(\"W shape is {0} in accGrad1\\n\".format(self.W.shape))\n",
    "        print(\"Wgrad shape is {0} in accGrad0\\n\".format(self.gradW.shape))\n",
    "        self.gradW = (input.T @ gradOutput).T\n",
    "        self.gradb = np.sum(gradOutput,axis=0)\n",
    "        print(\"Wgrad shape is {0} in accGrad1\\n\".format(self.gradW.shape))\n",
    "        self.W = np.subtract(self.W, np.multiply(self.learning_rate, self.gradW))\n",
    "        # there it tells me that gradW and W has 3,4 and 4,3 shape. Come on, grad created based on W shape\n",
    "        print(\"W shape is {0}in accGrad2\\n\".format(self.W.shape))\n",
    "        print(\"Wgrad shape is {0} in accGrad2\\n\".format(self.gradW.shape))\n",
    "        self.b = np.subtract(self.b, np.multiply(self.learning_rate, self.gradb))\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n",
      "W shape is (4, 3) in init1\n",
      "\n",
      "W shape is (4, 3) in init2\n",
      "\n",
      "Wgrad shape is (4, 3) in init\n",
      "\n",
      "W shape is (4, 3) in updateOutput\n",
      "\n",
      "1. check layer output: OK!\n",
      "W shape is (4, 3) in updateGrad\n",
      "\n",
      "Wgrad shape is (4, 3) in upgradeGrad1\n",
      "\n",
      "Wgrad shape is (4, 3) in upgradeGrad2\n",
      "\n",
      "2. check layer input grad: OK!\n",
      "W shape is (4, 3) in accGrad1\n",
      "\n",
      "Wgrad shape is (4, 3) in accGrad0\n",
      "\n",
      "Wgrad shape is (4, 3) in accGrad1\n",
      "\n",
      "W shape is (4, 3)in accGrad2\n",
      "\n",
      "Wgrad shape is (4, 3) in accGrad2\n",
      "\n",
      "[[ 41.35547    -15.495246    48.966015  ]\n",
      " [  9.275587    -0.68649626   1.5852559 ]\n",
      " [ -4.2775316    3.699715   -12.130545  ]\n",
      " [ 15.5450115  -17.468357    57.639904  ]]\n",
      "[[ 41.355465   -15.495246    48.966015  ]\n",
      " [  9.275588    -0.68649626   1.5852559 ]\n",
      " [ -4.2775316    3.6997147  -12.130545  ]\n",
      " [ 15.545012   -17.468357    57.639904  ]]\n",
      "3. check layer parameters grad. Weight: OK!\n",
      "3. check layer parameters grad. Bias: OK!\n",
      "Iter 1\n",
      "W shape is (4, 3) in init1\n",
      "\n",
      "W shape is (4, 3) in init2\n",
      "\n",
      "Wgrad shape is (4, 3) in init\n",
      "\n",
      "W shape is (4, 3) in updateOutput\n",
      "\n",
      "1. check layer output: OK!\n",
      "W shape is (4, 3) in updateGrad\n",
      "\n",
      "Wgrad shape is (4, 3) in upgradeGrad1\n",
      "\n",
      "Wgrad shape is (4, 3) in upgradeGrad2\n",
      "\n",
      "2. check layer input grad: OK!\n",
      "W shape is (4, 3) in accGrad1\n",
      "\n",
      "Wgrad shape is (4, 3) in accGrad0\n",
      "\n",
      "Wgrad shape is (4, 3) in accGrad1\n",
      "\n",
      "W shape is (4, 3)in accGrad2\n",
      "\n",
      "Wgrad shape is (4, 3) in accGrad2\n",
      "\n",
      "[[ 18.772789 -73.76709  -37.076042]\n",
      " [  8.58721  -53.935345 -26.08104 ]\n",
      " [ 49.95444  -84.63584  -48.219986]\n",
      " [ -9.755152 -29.422403 -11.340599]]\n",
      "[[ 18.77279  -73.76709  -37.076042]\n",
      " [  8.587212 -53.935345 -26.08104 ]\n",
      " [ 49.954445 -84.63584  -48.219986]\n",
      " [ -9.755152 -29.422403 -11.340599]]\n",
      "3. check layer parameters grad. Weight: OK!\n",
      "3. check layer parameters grad. Bias: OK!\n",
      "Iter 2\n",
      "W shape is (4, 3) in init1\n",
      "\n",
      "W shape is (4, 3) in init2\n",
      "\n",
      "Wgrad shape is (4, 3) in init\n",
      "\n",
      "W shape is (4, 3) in updateOutput\n",
      "\n",
      "1. check layer output: OK!\n",
      "W shape is (4, 3) in updateGrad\n",
      "\n",
      "Wgrad shape is (4, 3) in upgradeGrad1\n",
      "\n",
      "Wgrad shape is (4, 3) in upgradeGrad2\n",
      "\n",
      "2. check layer input grad: OK!\n",
      "W shape is (4, 3) in accGrad1\n",
      "\n",
      "Wgrad shape is (4, 3) in accGrad0\n",
      "\n",
      "Wgrad shape is (4, 3) in accGrad1\n",
      "\n",
      "W shape is (4, 3)in accGrad2\n",
      "\n",
      "Wgrad shape is (4, 3) in accGrad2\n",
      "\n",
      "[[-53.62847   62.16027   20.897495]\n",
      " [-68.4935    94.3409    31.492811]\n",
      " [-80.00612   88.44455   29.798035]\n",
      " [-14.699789  25.750095   8.526681]]\n",
      "[[-53.628475  62.160275  20.897495]\n",
      " [-68.4935    94.3409    31.492811]\n",
      " [-80.00611   88.44455   29.798035]\n",
      " [-14.69979   25.750097   8.526681]]\n",
      "3. check layer parameters grad. Weight: OK!\n",
      "3. check layer parameters grad. Bias: OK!\n",
      "Iter 3\n",
      "W shape is (4, 3) in init1\n",
      "\n",
      "W shape is (4, 3) in init2\n",
      "\n",
      "Wgrad shape is (4, 3) in init\n",
      "\n",
      "W shape is (4, 3) in updateOutput\n",
      "\n",
      "1. check layer output: OK!\n",
      "W shape is (4, 3) in updateGrad\n",
      "\n",
      "Wgrad shape is (4, 3) in upgradeGrad1\n",
      "\n",
      "Wgrad shape is (4, 3) in upgradeGrad2\n",
      "\n",
      "2. check layer input grad: OK!\n",
      "W shape is (4, 3) in accGrad1\n",
      "\n",
      "Wgrad shape is (4, 3) in accGrad0\n",
      "\n",
      "Wgrad shape is (4, 3) in accGrad1\n",
      "\n",
      "W shape is (4, 3)in accGrad2\n",
      "\n",
      "Wgrad shape is (4, 3) in accGrad2\n",
      "\n",
      "[[-66.787056  18.385248 -87.76497 ]\n",
      " [ 41.989727  57.855484   9.048237]\n",
      " [-71.20624  -19.377392 -67.66799 ]\n",
      " [-19.97271   34.662212 -45.62764 ]]\n",
      "[[-66.787056  18.385248 -87.76497 ]\n",
      " [ 41.989727  57.855484   9.048237]\n",
      " [-71.20624  -19.37739  -67.66799 ]\n",
      " [-19.972708  34.662212 -45.62764 ]]\n",
      "3. check layer parameters grad. Weight: OK!\n",
      "3. check layer parameters grad. Bias: OK!\n",
      "Iter 4\n",
      "W shape is (4, 3) in init1\n",
      "\n",
      "W shape is (4, 3) in init2\n",
      "\n",
      "Wgrad shape is (4, 3) in init\n",
      "\n",
      "W shape is (4, 3) in updateOutput\n",
      "\n",
      "1. check layer output: OK!\n",
      "W shape is (4, 3) in updateGrad\n",
      "\n",
      "Wgrad shape is (4, 3) in upgradeGrad1\n",
      "\n",
      "Wgrad shape is (4, 3) in upgradeGrad2\n",
      "\n",
      "2. check layer input grad: OK!\n",
      "W shape is (4, 3) in accGrad1\n",
      "\n",
      "Wgrad shape is (4, 3) in accGrad0\n",
      "\n",
      "Wgrad shape is (4, 3) in accGrad1\n",
      "\n",
      "W shape is (4, 3)in accGrad2\n",
      "\n",
      "Wgrad shape is (4, 3) in accGrad2\n",
      "\n",
      "[[  20.655512    20.04365    -10.728233 ]\n",
      " [  -0.9910822  -15.674628    10.59573  ]\n",
      " [ -86.2589    -100.43956     56.26892  ]\n",
      " [ -45.253124   -78.55403     47.23951  ]]\n",
      "[[  20.655514    20.04365    -10.728233 ]\n",
      " [  -0.9910822  -15.674628    10.59573  ]\n",
      " [ -86.2589    -100.43956     56.268917 ]\n",
      " [ -45.253124   -78.55403     47.23951  ]]\n",
      "3. check layer parameters grad. Weight: OK!\n",
      "3. check layer parameters grad. Bias: OK!\n",
      "Iter 5\n",
      "W shape is (4, 3) in init1\n",
      "\n",
      "W shape is (4, 3) in init2\n",
      "\n",
      "Wgrad shape is (4, 3) in init\n",
      "\n",
      "W shape is (4, 3) in updateOutput\n",
      "\n",
      "1. check layer output: OK!\n",
      "W shape is (4, 3) in updateGrad\n",
      "\n",
      "Wgrad shape is (4, 3) in upgradeGrad1\n",
      "\n",
      "Wgrad shape is (4, 3) in upgradeGrad2\n",
      "\n",
      "2. check layer input grad: FAILED:\n",
      "  expected=[[ 1.0169905e+00 -4.1093044e+00  9.3623996e-01]\n",
      " [ 9.9876171e-01 -7.0998330e+00  2.3865700e-03]]\n",
      "  actual=[[ 1.0169904e+00 -4.1093044e+00  9.3624002e-01]\n",
      " [ 9.9876159e-01 -7.0998330e+00  2.3866408e-03]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "2. check layer input grad: FAILED:\n  expected=[[ 1.0169905e+00 -4.1093044e+00  9.3623996e-01]\n [ 9.9876171e-01 -7.0998330e+00  2.3865700e-03]]\\  actual=[[ 1.0169904e+00 -4.1093044e+00  9.3624002e-01]\n [ 9.9876159e-01 -7.0998330e+00  2.3866408e-03]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4f2bf500e784>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0massertAlmostEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_bias_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'3. check layer parameters grad. Bias'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mtest_Linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-4f2bf500e784>\u001b[0m in \u001b[0;36mtest_Linear\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch_layer_output_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mtorch_layer_grad_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_input_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0massertAlmostEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_layer_grad_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_layer_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2. check layer input grad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# 3. check layer parameters grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f7a9542a9d61>\u001b[0m in \u001b[0;36massertAlmostEqual\u001b[0;34m(expected, actual, msg, rtol, atol)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{msg}: FAILED:\\n  expected={expected}\\n  actual={actual}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{msg}: FAILED:\\n  expected={expected}\\  actual={actual}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0massertTrue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"???\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 2. check layer input grad: FAILED:\n  expected=[[ 1.0169905e+00 -4.1093044e+00  9.3623996e-01]\n [ 9.9876171e-01 -7.0998330e+00  2.3865700e-03]]\\  actual=[[ 1.0169904e+00 -4.1093044e+00  9.3624002e-01]\n [ 9.9876159e-01 -7.0998330e+00  2.3866408e-03]]"
     ]
    }
   ],
   "source": [
    "def test_Linear():\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        batch_size, n_in, n_out = 2, 3, 4\n",
    "        for i in range(100):\n",
    "            print(f\"Iter {i}\")\n",
    "            # layers initialization\n",
    "            torch_layer = torch.nn.Linear(n_in, n_out)\n",
    "            custom_layer = Linear(n_in, n_out)\n",
    "            custom_layer.W = torch_layer.weight.data.numpy()\n",
    "            custom_layer.b = torch_layer.bias.data.numpy()\n",
    "\n",
    "            layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
    "            next_layer_grad = np.random.uniform(-10, 10, (batch_size, n_out)).astype(np.float32)\n",
    "\n",
    "            # 1. check layer output\n",
    "            custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "            layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "            torch_layer_output_var = torch_layer(layer_input_var)\n",
    "            assertAlmostEqual(torch_layer_output_var.data.numpy(), custom_layer_output, msg=\"1. check layer output\")\n",
    "        \n",
    "            # 2. check layer input grad\n",
    "            custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "            torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "            torch_layer_grad_var = layer_input_var.grad\n",
    "            assertAlmostEqual(torch_layer_grad_var.data.numpy(), custom_layer_grad, msg='2. check layer input grad')\n",
    "\n",
    "            # 3. check layer parameters grad\n",
    "            custom_layer.accGradParameters(layer_input, next_layer_grad)\n",
    "            weight_grad = custom_layer.gradW\n",
    "            bias_grad = custom_layer.gradb\n",
    "            torch_weight_grad = torch_layer.weight.grad.data.numpy()\n",
    "            torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
    "            print(torch_weight_grad)\n",
    "            print(weight_grad)\n",
    "            assertAlmostEqual(torch_weight_grad, weight_grad, msg='3. check layer parameters grad. Weight')\n",
    "            assertAlmostEqual(torch_bias_grad, bias_grad, msg='3. check layer parameters grad. Bias')\n",
    "            \n",
    "test_Linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SoftMax\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**\n",
    "\n",
    "$\\text{softmax}(x)_i = \\frac{\\exp x_i} {\\sum_j \\exp x_j}$\n",
    "\n",
    "Recall that $\\text{softmax}(x) == \\text{softmax}(x - \\text{const})$. It makes possible to avoid computing exp() from large argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(SoftMax, self).__init__()\n",
    "# softmax(ùë•)==softmax(ùë•‚àíconst)\n",
    "    \n",
    "    def softmax(input):\n",
    "        exp = np.exp(input - np.max(input, axis=1, keepdims=True))\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        print(input.shape) # (2,4)\n",
    "        # start with normalization for numerical stability\n",
    "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "        self.output = SoftMax.softmax(self.output)\n",
    "        # Your code goes here. ################################################\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        # ai(1‚àíai) ; since i=j\n",
    "        # ‚àíaiaj ; since i!=j\n",
    "        grad = np.zeros_like(self.output)\n",
    "        #for i in range(self.output.shape[0]):\n",
    "        #    for j in range(self.output.shape[1]):\n",
    "        #        sum1 = np.sum(self.output[i], axis=1, keepdims=True)\n",
    "        #       sum2 = np.sum(self.output[j], axis=1, keepdims=True)\n",
    "        #        if(i==j):            \n",
    "        #           grad[i][j] =  sum1 * (1. -  sum1)\n",
    "        #        else:\n",
    "        #            grad[i][j] = -sum1 * sum2\n",
    "        #np.fill_diagonal(grad, self.output[i] * (1. - self.output[i]))\n",
    "        #grad = -self.output[i] * self.output[j]\n",
    "        self.gradInput = gradOutput * grad\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SoftMax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_SoftMax. Iter 0\n",
      "(2, 4)\n",
      "1. check layer output: OK!\n",
      "2. check layer input grad: FAILED:\n",
      "  expected=[[-7.5124099e-06 -1.3294163e+00  1.3313441e+00 -1.9205434e-03]\n",
      " [-4.0679720e-06 -7.1062163e-07  6.3931580e-09  4.7683648e-06]]\n",
      "  actual=[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "2. check layer input grad: FAILED:\n  expected=[[-7.5124099e-06 -1.3294163e+00  1.3313441e+00 -1.9205434e-03]\n [-4.0679720e-06 -7.1062163e-07  6.3931580e-09  4.7683648e-06]]\\  actual=[[0. 0. 0. 0.]\n [0. 0. 0. 0.]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-424785bdc003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0massertAlmostEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_layer_grad_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_layer_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2. check layer input grad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtest_SoftMax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-424785bdc003>\u001b[0m in \u001b[0;36mtest_SoftMax\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtorch_layer_output_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtorch_layer_grad_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_input_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0massertAlmostEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_layer_grad_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_layer_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2. check layer input grad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mtest_SoftMax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f7a9542a9d61>\u001b[0m in \u001b[0;36massertAlmostEqual\u001b[0;34m(expected, actual, msg, rtol, atol)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{msg}: FAILED:\\n  expected={expected}\\n  actual={actual}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{msg}: FAILED:\\n  expected={expected}\\  actual={actual}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0massertTrue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"???\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 2. check layer input grad: FAILED:\n  expected=[[-7.5124099e-06 -1.3294163e+00  1.3313441e+00 -1.9205434e-03]\n [-4.0679720e-06 -7.1062163e-07  6.3931580e-09  4.7683648e-06]]\\  actual=[[0. 0. 0. 0.]\n [0. 0. 0. 0.]]"
     ]
    }
   ],
   "source": [
    "def test_SoftMax():\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    batch_size, n_in = 2, 4\n",
    "    for i in range(100):\n",
    "        print(f\"test_SoftMax. Iter {i}\")\n",
    "        # layers initialization\n",
    "        torch_layer = torch.nn.Softmax(dim=1)\n",
    "        custom_layer = SoftMax()\n",
    "\n",
    "        layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
    "        next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n",
    "        next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n",
    "        next_layer_grad = next_layer_grad.clip(1e-5,1.)\n",
    "        next_layer_grad = 1. / next_layer_grad\n",
    "\n",
    "        # 1. check layer output\n",
    "        custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "        torch_layer_output_var = torch_layer(layer_input_var)\n",
    "        assertAlmostEqual(torch_layer_output_var.data.numpy(), custom_layer_output, msg='1. check layer output')\n",
    "\n",
    "        # 2. check layer input grad\n",
    "        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "        torch_layer_grad_var = layer_input_var.grad\n",
    "        assertAlmostEqual(torch_layer_grad_var.data.numpy(), custom_layer_grad, msg='2. check layer input grad')\n",
    "        \n",
    "test_SoftMax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LogSoftMax\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**\n",
    "\n",
    "$\\text{logsoftmax}(x)_i = \\log\\text{softmax}(x)_i = x_i - \\log {\\sum_j \\exp x_j}$\n",
    "\n",
    "The main goal of this layer is to be used in computation of log-likelihood loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogSoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(LogSoftMax, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        # start with normalization for numerical stability\n",
    "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "        self.output = np.subtract(self.output,np.log(np.sum(np.exp(self.output),axis=1,keepdim=True)))\n",
    "        # Your code goes here. ################################################\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"LogSoftMax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LogSoftMax():\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    batch_size, n_in = 2, 4\n",
    "    for i in range(100):\n",
    "        print(f\"test_LogSoftMax. Iter {i}\")\n",
    "        # layers initialization\n",
    "        torch_layer = torch.nn.LogSoftmax(dim=1)\n",
    "        custom_layer = LogSoftMax()\n",
    "\n",
    "        layer_input = np.random.uniform(-10, 10, (batch_size, n_in)).astype(np.float32)\n",
    "        next_layer_grad = np.random.random((batch_size, n_in)).astype(np.float32)\n",
    "        next_layer_grad /= next_layer_grad.sum(axis=-1, keepdims=True)\n",
    "\n",
    "        # 1. check layer output\n",
    "        custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "        torch_layer_output_var = torch_layer(layer_input_var)\n",
    "        assertAlmostEqual(torch_layer_output_var.data.numpy(), custom_layer_output, msg='1. check layer output')\n",
    "\n",
    "        # 2. check layer input grad\n",
    "        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "        torch_layer_grad_var = layer_input_var.grad\n",
    "        assertAlmostEqual(torch_layer_grad_var.data.numpy(), custom_layer_grad, msg='2. check layer input grad')\n",
    "        \n",
    "test_LogSoftMax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch normalization\n",
    "One of the most significant recent ideas that impacted NNs a lot is [**Batch normalization**](http://arxiv.org/abs/1502.03167). The idea is simple, yet effective: the features should be whitened ($mean = 0$, $std = 1$) all the way through NN. This improves the convergence for deep models letting it train them for days but not weeks. **You are** to implement the first part of the layer: features normalization. The second part (`ChannelwiseScaling` layer) is implemented below.\n",
    "\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**\n",
    "\n",
    "The layer should work as follows. While training (`self.training == True`) it transforms input as $$y = \\frac{x - \\mu}  {\\sqrt{\\sigma + \\epsilon}}$$\n",
    "where $\\mu$ and $\\sigma$ - mean and variance of feature values in **batch** and $\\epsilon$ is just a small number for numericall stability. Also during training, layer should maintain exponential moving average values for mean and variance: \n",
    "```\n",
    "    self.moving_mean = self.moving_mean * alpha + batch_mean * (1 - alpha)\n",
    "    self.moving_variance = self.moving_variance * alpha + batch_variance * (1 - alpha)\n",
    "```\n",
    "During testing (`self.training == False`) the layer normalizes input using moving_mean and moving_variance. \n",
    "\n",
    "Note that decomposition of batch normalization on normalization itself and channelwise scaling here is just a common **implementation** choice. In general \"batch normalization\" always assumes normalization + scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization(Module):\n",
    "    EPS = 1e-3\n",
    "    def __init__(self, alpha = 0.):\n",
    "        super(BatchNormalization, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.moving_mean = None \n",
    "        self.moving_variance = None\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        # use self.EPS please\n",
    "        self.output = input\n",
    "        #print(dir(input))\n",
    "        if (self.training):\n",
    "            batch_mean = input.mean()\n",
    "            batch_variance = input.var() #input.var()\n",
    "            self.moving_mean = self.moving_mean * self.alpha + batch_mean * (1 - self.alpha)\n",
    "            self.moving_variance = self.moving_variance * self.alpha + batch_variance * (1 - self.alpha)\n",
    "            \n",
    "            self.output = np.subtract(input,self.moving_mean) / np.sqrt(self.moving_mean + self.EPS)\n",
    "        #else:\n",
    "            \n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        #if (self.training):\n",
    "        \n",
    "        #else:\n",
    "        self.gradInput = gradOutput    \n",
    "        # Your code goes here. ################################################\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"BatchNormalization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "???: FAILED:\n",
      "  expected=[[ 1.41016996e+00  7.32519150e-01  4.50298339e-01 -9.46392715e-01\n",
      "  -1.47206545e+00 -1.37083197e+00  1.37724662e+00  3.88500780e-01\n",
      "   5.41669905e-01 -1.58092630e+00  1.23230290e+00  1.08739460e+00\n",
      "  -6.12358093e-01 -1.12429333e+00 -1.24007118e+00 -7.80916631e-01]\n",
      " [ 2.95836106e-03 -2.94019282e-01 -6.19553506e-01  4.92805600e-01\n",
      "  -1.52739215e+00 -6.47969484e-01 -5.03135085e-01 -4.42811698e-02\n",
      "   8.19093764e-01 -9.12917614e-01 -2.08963335e-01  2.73858875e-01\n",
      "  -1.26506019e+00  3.70364130e-01 -1.28933299e+00 -1.57288957e+00]\n",
      " [ 1.40412831e+00  1.53185034e+00  1.18018794e+00 -4.77235675e-01\n",
      "  -1.66762042e+00  5.62936842e-01 -2.25522816e-01 -1.04095721e+00\n",
      "  -2.24344641e-01 -1.52943671e+00  1.04066312e+00 -8.56940687e-01\n",
      "   1.15892267e+00 -6.68276966e-01  4.75193001e-02  2.19079480e-02]\n",
      " [-1.11995554e+00  1.54537308e+00  1.06442773e+00  1.52727723e+00\n",
      "   1.00523961e+00  2.96310157e-01  1.58679175e+00 -1.14105034e+00\n",
      "  -1.30086732e+00 -1.48900783e+00 -8.06452513e-01 -4.16675121e-01\n",
      "  -3.80178988e-01  1.14694810e+00 -5.77087879e-01 -8.58089745e-01]\n",
      " [ 6.22245148e-02 -1.28967035e+00  1.15861082e+00 -1.20361042e+00\n",
      "   1.31391525e+00  8.34747970e-01 -1.13384676e+00 -1.38861549e+00\n",
      "   9.28063095e-01  9.78892982e-01  4.70346242e-01  8.80061150e-01\n",
      "  -1.15648842e+00 -5.04126310e-01 -1.49836469e+00  1.06950259e+00]\n",
      " [ 3.28504682e-01 -6.39724970e-01 -1.41184616e+00 -4.57128316e-01\n",
      "  -9.04775739e-01  7.03065097e-01  5.17146409e-01  1.24215245e+00\n",
      "  -3.06963593e-01 -1.21161675e+00  4.20491040e-01  8.44523013e-01\n",
      "   7.60565996e-01  9.44122612e-01 -5.29612228e-02 -5.74826635e-02]\n",
      " [-3.18206370e-01 -1.68484068e+00 -1.25756729e+00 -1.33975708e+00\n",
      "   1.38768137e-01 -5.79373121e-01  3.18774097e-02  1.30288339e+00\n",
      "  -1.10905623e+00 -1.26965895e-01  5.54302752e-01 -9.58559096e-01\n",
      "  -1.14493942e+00 -7.45374858e-01 -1.32491231e+00  1.28999960e+00]\n",
      " [ 9.39090848e-01  3.95218790e-01  1.39964759e+00  1.09843326e+00\n",
      "  -1.36954582e+00  1.20632052e+00  1.47644922e-01  1.00412893e+00\n",
      "   1.21817541e+00 -4.71544147e-01 -1.48736143e+00 -9.61484075e-01\n",
      "   2.32666001e-01  1.10930169e+00  1.35040438e+00 -1.76525998e+00]\n",
      " [-4.33227643e-02 -3.43743563e-01 -8.60095143e-01 -1.06053889e+00\n",
      "  -8.63091767e-01  1.36182117e+00 -6.65505886e-01  1.42863214e-01\n",
      "   5.23486614e-01 -3.01357239e-01  1.23822451e+00  1.52802396e+00\n",
      "  -4.57165807e-01 -1.68746710e-02 -7.90785372e-01 -8.45156848e-01]\n",
      " [-1.60878682e+00  3.13658476e-01  1.16290532e-01 -1.27645493e+00\n",
      "  -1.06081378e+00  1.25482881e+00 -9.80176806e-01 -9.72758174e-01\n",
      "  -2.44940564e-01  2.01879978e+00 -1.06984508e+00  5.44060051e-01\n",
      "   1.54882884e+00 -9.28341269e-01  8.43595505e-01 -5.70529401e-01]\n",
      " [ 3.58262867e-01  3.95649731e-01  2.31463164e-01 -1.15391743e+00\n",
      "   8.05652916e-01 -5.59533298e-01 -1.17973459e+00 -1.28342819e+00\n",
      "   1.20048836e-01  8.69629264e-01 -1.78298116e+00  1.62234041e-03\n",
      "  -5.56657612e-01  5.02471805e-01 -1.27463794e+00  4.99452710e-01]\n",
      " [-4.53013629e-01  1.43296969e+00 -1.15445685e+00 -3.62144530e-01\n",
      "  -1.61463833e+00  1.30556357e+00  1.41924441e+00 -6.35451913e-01\n",
      "   3.68643790e-01  1.39055729e+00 -7.93898702e-02  6.11306690e-02\n",
      "  -4.96236235e-01 -1.43578756e+00  1.48994422e+00  1.19305325e+00]\n",
      " [ 3.60891342e-01 -6.11904263e-01 -4.17782009e-01  8.53060544e-01\n",
      "   1.01289427e+00  1.18941927e+00  1.05256987e+00  5.10586858e-01\n",
      "  -1.70328653e+00 -1.05482686e+00  1.00661039e+00  3.21358711e-01\n",
      "  -1.41163647e+00 -1.40640569e+00  5.96090555e-01 -1.77151966e+00]\n",
      " [-1.19939613e+00  1.05542779e-01  7.74761438e-01  6.19439125e-01\n",
      "  -1.24314022e+00  6.49244547e-01 -9.88877952e-01 -4.34172124e-01\n",
      "   6.79903805e-01  7.65443981e-01  8.50580871e-01  4.94837910e-01\n",
      "   7.88231611e-01 -1.43377936e+00 -5.35161257e-01 -9.10179675e-01]\n",
      " [-9.24594164e-01  1.55709398e+00 -2.65051574e-01  1.37745655e+00\n",
      "   1.21091820e-01  9.04441357e-01  9.55426414e-03  3.16260219e-01\n",
      "  -2.33912706e-01 -9.29444611e-01  4.49613094e-01 -7.82401145e-01\n",
      "  -1.35214984e+00  5.03523350e-01 -1.26414239e+00  1.32562959e+00]\n",
      " [ 1.42078876e+00  1.35816228e+00 -3.44879210e-01 -1.39018703e+00\n",
      "   1.11753559e+00 -2.27831841e-01  1.75526357e+00  1.47013462e+00\n",
      "   1.06316376e+00 -5.59403419e-01 -6.17412746e-01  1.15075493e+00\n",
      "  -2.00868532e-01 -1.16759050e+00  1.88007846e-01  1.31137955e+00]\n",
      " [ 5.68783939e-01  1.78508818e-01 -1.29485512e+00  5.02764702e-01\n",
      "   1.32453394e+00 -1.11758614e+00  6.85921833e-02  1.21279311e+00\n",
      "   6.59312785e-01  9.42183495e-01  3.86455745e-01 -5.15596747e-01\n",
      "  -2.92662948e-01  1.07891977e+00  1.15681589e+00  1.08264375e+00]\n",
      " [ 1.28637016e+00 -2.23820228e-02  1.12244241e-01  1.08145690e+00\n",
      "   1.84213027e-01  6.17705226e-01  1.11245263e+00  1.25048494e+00\n",
      "  -7.89896369e-01 -2.56770849e-01 -1.53818953e+00  2.25952566e-01\n",
      "  -1.30640531e+00 -1.27996191e-01  1.33864954e-01 -8.39525521e-01]\n",
      " [ 2.21252427e-01 -1.66745687e+00 -1.50305724e+00  1.15819585e+00\n",
      "  -7.87396252e-01 -1.15780723e+00  8.33158121e-02  8.92396629e-01\n",
      "  -1.22948813e+00  6.65693939e-01 -1.56549966e+00 -1.55886626e+00\n",
      "   6.42833352e-01  1.35451153e-01  4.96377140e-01  6.15848184e-01]\n",
      " [ 1.49321604e+00 -5.41970087e-03 -5.09142578e-01  1.07164097e+00\n",
      "  -1.08701479e+00 -1.94516957e-01 -1.58628154e+00 -1.32945120e+00\n",
      "   1.45765293e+00  1.46052504e+00  3.65865499e-01 -3.47954124e-01\n",
      "  -7.65982866e-01 -1.21342766e+00 -9.84443605e-01  3.02398372e-02]\n",
      " [ 6.30119741e-01  4.86886114e-01 -6.58860803e-01  1.57579315e+00\n",
      "   4.79052186e-01  1.61825076e-01  4.19944465e-01 -1.53099254e-01\n",
      "  -1.11467373e+00 -3.29917789e-01  5.61561584e-01 -1.68524861e+00\n",
      "  -9.91126060e-01 -1.60115123e+00 -1.78574336e+00  1.04419661e+00]\n",
      " [ 5.93984365e-01 -1.49544507e-01 -1.29256654e+00  1.13183148e-01\n",
      "  -4.07564849e-01 -1.01530647e+00 -2.49227628e-01 -2.16043025e-01\n",
      "   2.09846586e-01  7.11212158e-01 -1.69215405e+00 -4.64345008e-01\n",
      "   1.01467180e+00  3.79660563e-03  1.33418548e+00  3.92690778e-01]\n",
      " [-1.19237137e+00 -1.53037310e+00  6.02585554e-01 -1.35528433e+00\n",
      "  -3.10103670e-02  1.35354602e+00  2.83579081e-01 -2.46879801e-01\n",
      "   3.08570981e-01  5.15905917e-02 -1.09703377e-01  1.45690715e+00\n",
      "   7.13280514e-02  1.61197674e+00  1.52105665e+00 -1.14000404e+00]\n",
      " [-1.50150323e+00 -1.42702007e+00 -1.56961691e+00 -1.14080465e+00\n",
      "   2.95005649e-01 -1.33035910e+00 -6.81409657e-01  1.11582708e+00\n",
      "  -1.92229426e+00  1.38028586e+00 -9.43962336e-01 -1.33353257e+00\n",
      "   1.29354393e+00  4.45490211e-01  1.41443312e+00  6.45580411e-01]\n",
      " [ 9.23763812e-01 -8.06898475e-01 -1.01554036e+00  9.30916309e-01\n",
      "   7.10200608e-01  1.50881255e+00 -3.29113245e-01 -2.95073271e-01\n",
      "   7.87563801e-01 -3.86499196e-01  1.10846639e+00  1.17541599e+00\n",
      "   2.40087569e-01  8.73567283e-01  9.44282949e-01 -1.44683027e+00]\n",
      " [ 1.25106192e+00 -4.32174429e-02  1.24303710e+00 -4.28500354e-01\n",
      "   1.00757301e+00 -3.48223358e-01 -1.84067416e+00  1.29636538e+00\n",
      "  -1.67757213e+00 -4.66657221e-01  1.16952550e+00  1.48789370e+00\n",
      "   8.08413208e-01  4.55652028e-01 -2.26405397e-01 -8.17442417e-01]\n",
      " [-6.44858658e-01  5.29039443e-01  9.85229015e-01  1.06025207e+00\n",
      "   6.52473330e-01 -1.26853812e+00 -2.13586148e-02 -1.23334968e+00\n",
      "  -2.87821610e-02 -1.07844472e-02  9.72292602e-01 -5.44664145e-01\n",
      "  -9.87213492e-01 -1.26063287e+00  9.70931590e-01  2.58674026e-01]\n",
      " [-1.39657509e+00 -1.48405612e+00  8.06338608e-01 -1.20925438e+00\n",
      "   7.60580420e-01  6.30909026e-01 -1.57539988e+00 -1.15195525e+00\n",
      "   1.54397500e+00 -2.61665255e-01 -6.63134634e-01  1.02081764e+00\n",
      "   2.27920032e+00  1.69908381e+00  9.39828515e-01 -5.42463362e-01]\n",
      " [-1.45479167e+00  8.86998117e-01  3.10213894e-01 -9.95984375e-02\n",
      "   1.04388976e+00 -1.20679784e+00 -2.81124450e-02 -1.37121546e+00\n",
      "  -3.19752187e-01 -1.44769406e+00 -1.45963526e+00 -1.33569694e+00\n",
      "   1.10654569e+00  8.56623054e-01  2.89616942e-01  1.39752567e+00]\n",
      " [-4.92210567e-01 -7.94316828e-01  1.38968980e+00 -7.33032286e-01\n",
      "   1.23456860e+00 -1.51267731e+00  1.76739264e+00 -1.27631259e+00\n",
      "   1.20037150e+00  3.10634613e-01  1.30522394e+00 -1.48391163e+00\n",
      "   7.31360018e-01  1.64045715e+00  5.91070466e-02  2.95693517e-01]\n",
      " [ 5.67855239e-01 -2.16712832e-01  5.50868690e-01  4.05858457e-01\n",
      "   1.02646625e+00 -1.40986013e+00 -8.24418545e-01  1.43072331e+00\n",
      "   1.19720757e+00  4.19069082e-02  1.25984386e-01 -7.93894947e-01\n",
      "  -7.07645535e-01 -1.34665519e-01 -5.90095639e-01  1.44237578e-01]\n",
      " [-1.47384083e+00  1.56182992e+00  1.79897726e+00  7.65307069e-01\n",
      "  -1.97584689e-01 -5.94284832e-01  1.18018043e+00  6.37992740e-01\n",
      "  -1.42092049e+00  1.74007988e+00  7.66174614e-01  1.48515773e+00\n",
      "   1.40757704e+00  3.90974283e-01 -3.41916680e-01  1.30003476e+00]]\n",
      "  actual=[[ 72.12897     37.10405     15.7523     -55.13009    -55.133953\n",
      "  -70.81297     58.5914      16.145678    33.273388   -76.81792\n",
      "   75.20286     53.189465   -46.11118    -50.997578   -50.744637\n",
      "  -31.394217  ]\n",
      " [  3.9179523  -10.944458   -33.47808     17.865194   -57.776295\n",
      "  -33.33147    -21.446657    -7.0811925   45.620388   -48.13935\n",
      "    2.2330067   14.752427   -72.67586     17.175323   -52.807247\n",
      "  -69.69715   ]\n",
      " [ 71.83611     74.51782     49.338955   -31.334723   -64.47344\n",
      "   29.45583     -9.630149   -60.57156     -0.81878185 -74.6074\n",
      "   65.50033    -38.674343    25.979166   -30.19819      3.1671708\n",
      "    7.433544  ]\n",
      " [-50.512447    75.15077     44.012123    70.332985    63.179474\n",
      "   15.6308365   67.51064    -65.94343    -48.730396   -72.87173\n",
      "  -28.017263   -17.873161   -36.66159     52.596138   -22.985323\n",
      "  -35.126614  ]\n",
      " [  6.7907286  -57.547245    48.346058   -68.17602     77.921486\n",
      "   43.54964    -48.292774   -79.22994     50.470165    33.07879\n",
      "   36.625763    43.3936     -68.25704    -22.711107   -61.559464\n",
      "   58.099346  ]\n",
      " [ 19.697985   -27.125677   -69.93627    -30.31489    -28.040804\n",
      "   36.721684    21.981413    61.960106    -4.495813   -60.962933\n",
      "   34.101646    41.714535     9.766227    43.345055    -1.0399792\n",
      "    3.5939016 ]\n",
      " [-11.649695   -76.04372    -62.83697    -75.08129     21.797728\n",
      "  -29.774647     1.3260391   65.21946    -40.19367    -14.397317\n",
      "   40.8764     -43.4755     -67.787      -33.714706   -54.29696\n",
      "   68.76345   ]\n",
      " [ 49.2946      21.316254    59.437622    48.58227    -50.23772\n",
      "   62.81623      6.2536592   49.185677    63.381878   -29.190554\n",
      "  -62.490997   -43.613693   -11.71907     50.879047    57.719368\n",
      "  -79.00094   ]\n",
      " [  1.674592   -13.271871   -44.54686    -60.919514   -26.050024\n",
      "   70.87917    -28.357935     2.9626138   32.464127   -21.884188\n",
      "   75.50266     74.007835   -39.79492     -0.48704565 -31.932901\n",
      "  -34.50113   ]\n",
      " [-74.20731     17.498714     0.38257343 -71.87064    -35.49301\n",
      "   65.33146    -41.751842   -56.911404    -1.735422    77.72348\n",
      "  -41.352562    27.518593    41.848175   -42.059998    36.499123\n",
      "  -21.219059  ]\n",
      " [ 21.140436    21.336424     5.6823664  -65.65562     53.647427\n",
      "  -28.745922   -50.245975   -73.58467     14.50876     28.38794\n",
      "  -77.457924     1.8900954  -43.844193    23.200895   -52.191956\n",
      "   30.529491  ]\n",
      " [-18.184135    69.88958    -58.09223    -25.497366   -61.943077\n",
      "   67.96213     60.379025   -38.808594    25.572704    50.75213\n",
      "    8.79318      4.7016788  -41.38507    -65.205154    63.561943\n",
      "   64.074745  ]\n",
      " [ 21.267845   -25.823492   -24.193348    36.13711     63.54505\n",
      "   61.939877    44.7716      22.697887   -66.64043    -54.231716\n",
      "   63.77627     16.996647   -78.64145    -63.86502     26.136017\n",
      "  -79.30369   ]\n",
      " [-54.363132     7.7575803   30.682816    24.287971   -44.200726\n",
      "   33.93101    -42.122204   -28.006155    39.425613    23.915117\n",
      "   55.87666     25.192999    10.892204   -65.11356    -21.229841\n",
      "  -37.64589   ]\n",
      " [-41.042805    75.69939    -17.165293    62.73417     20.953526\n",
      "   47.163345     0.37585893  12.268615    -1.244617   -48.848877\n",
      "   35.576065   -35.15258    -76.220375    23.248857   -51.752506\n",
      "   70.48666   ]\n",
      " [ 72.643684    66.38812    -20.838646   -77.63907     68.54261\n",
      "  -11.546707    74.68161     74.195625    56.482944   -32.96248\n",
      "  -18.446375    56.18304    -29.363735   -52.972404     9.049469\n",
      "   69.79747   ]\n",
      " [ 31.344912    11.1728525  -64.55281     18.370316    78.42863\n",
      "  -57.681793     2.888796    60.384426    38.50919     31.502796\n",
      "   32.378475   -22.546902   -33.099728    49.493294    49.613758\n",
      "   58.734905  ]\n",
      " [ 66.12809      1.7698882    0.19637921  47.72124     23.968128\n",
      "   32.29565     47.3205      62.4073     -25.989174   -19.97003\n",
      "  -65.06438     12.489002   -74.35859     -5.555415     6.78249\n",
      "  -34.228775  ]\n",
      " [ 14.499204   -75.23005    -74.13345     51.613396   -22.434887\n",
      "  -59.76732      3.515504    43.18914    -45.5536      19.632704\n",
      "  -66.44706    -71.83814      4.974559     6.4606953   21.960985\n",
      "   36.158836  ]\n",
      " [ 76.15441      2.5638325  -28.397402    47.223377   -36.744347\n",
      "   -9.819282   -67.55057    -76.054665    74.04004     53.75594\n",
      "   31.33601    -14.626307   -52.363636   -55.063087   -40.04143\n",
      "    7.8365073 ]\n",
      " [ 34.31801     25.606867   -35.286846    72.79368     38.049328\n",
      "    8.657584    17.844032   -12.921325   -40.44368    -23.110332\n",
      "   41.243904   -77.80932    -61.526867   -72.74757    -73.592094\n",
      "   56.875454  ]\n",
      " [ 32.566437    -4.1821227  -64.447495    -1.3890303   -4.2945476\n",
      "  -52.378445   -10.639139   -16.299437    18.505291    21.586868\n",
      "  -72.85944    -20.125416    20.108217     0.45579308  57.040276\n",
      "   25.366064  ]\n",
      " [-54.022625   -68.81366     22.75996    -75.86883     13.68929\n",
      "   70.45009     12.039668   -17.954409    22.899109    -6.7316313\n",
      "    7.2584376   70.64779    -18.285454    73.80659     64.864624\n",
      "  -48.7611    ]\n",
      " [-69.00701    -63.97608    -77.196266   -64.99055     29.259462\n",
      "  -68.71439    -29.034878    55.180374   -76.38756     50.31116\n",
      "  -34.979248   -61.191833    31.458187    20.601904    60.40027\n",
      "   37.596806  ]\n",
      " [ 48.551662   -34.950455   -51.69984     40.085907    49.088726\n",
      "   78.50089    -14.039453   -20.540892    44.217117   -25.539452\n",
      "   68.93314     57.3482     -11.417016    40.12695     40.71494\n",
      "  -63.60042   ]\n",
      " [ 64.41661      0.7946582   52.231026   -28.862896    63.290913\n",
      "  -17.789188   -78.37873     64.869644   -65.49599    -28.980751\n",
      "   72.024506    72.11181     11.713585    21.065395    -8.30214\n",
      "  -33.160748  ]\n",
      " [-27.483337    27.57991     40.36771     46.645744    46.331734\n",
      "  -65.50888     -0.93994135 -70.89702      7.884903    -9.409481\n",
      "   62.038803   -23.920244   -61.36763    -57.216167    41.830723\n",
      "   18.884485  ]\n",
      " [-63.920887   -66.64573     32.135876   -68.46228     51.494812\n",
      "   32.980286   -67.08739    -66.52869     77.88188    -20.180153\n",
      "  -20.761229    50.043907    71.573975    77.77963     40.528427\n",
      "  -19.861677  ]\n",
      " [-66.74279     44.33465      9.306165   -12.181187    65.02536\n",
      "  -62.307552    -1.2274168  -78.296104    -5.0649805  -71.09807\n",
      "  -61.08725    -61.29409     23.847445    39.354107    13.303872\n",
      "   73.96384   ]\n",
      " [-20.084106   -34.361557    58.979404   -44.308575    74.13198\n",
      "  -78.16786     75.19788    -73.20279     62.589497     4.3894997\n",
      "   78.89477    -68.29677      8.577554    75.105606     3.6523533\n",
      "   20.674894  ]\n",
      " [ 31.299896    -7.326027    20.380154    13.455288    64.19324\n",
      "  -72.83664    -35.12202     72.08047     62.448685    -7.1473656\n",
      "   19.19107    -35.69563    -49.989334    -5.8596106  -23.52996\n",
      "   13.349887  ]\n",
      " [-67.666145    75.92105     77.8132      31.686308     5.733879\n",
      "  -30.54784     50.203316    29.5356     -54.073467    65.75764\n",
      "   51.603256    71.98254     36.09929     18.115376   -13.138633\n",
      "   69.248795  ]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "???: FAILED:\n  expected=[[ 1.41016996e+00  7.32519150e-01  4.50298339e-01 -9.46392715e-01\n  -1.47206545e+00 -1.37083197e+00  1.37724662e+00  3.88500780e-01\n   5.41669905e-01 -1.58092630e+00  1.23230290e+00  1.08739460e+00\n  -6.12358093e-01 -1.12429333e+00 -1.24007118e+00 -7.80916631e-01]\n [ 2.95836106e-03 -2.94019282e-01 -6.19553506e-01  4.92805600e-01\n  -1.52739215e+00 -6.47969484e-01 -5.03135085e-01 -4.42811698e-02\n   8.19093764e-01 -9.12917614e-01 -2.08963335e-01  2.73858875e-01\n  -1.26506019e+00  3.70364130e-01 -1.28933299e+00 -1.57288957e+00]\n [ 1.40412831e+00  1.53185034e+00  1.18018794e+00 -4.77235675e-01\n  -1.66762042e+00  5.62936842e-01 -2.25522816e-01 -1.04095721e+00\n  -2.24344641e-01 -1.52943671e+00  1.04066312e+00 -8.56940687e-01\n   1.15892267e+00 -6.68276966e-01  4.75193001e-02  2.19079480e-02]\n [-1.11995554e+00  1.54537308e+00  1.06442773e+00  1.52727723e+00\n   1.00523961e+00  2.96310157e-01  1.58679175e+00 -1.14105034e+00\n  -1.30086732e+00 -1.48900783e+00 -8.06452513e-01 -4.16675121e-01\n  -3.80178988e-01  1.14694810e+00 -5.77087879e-01 -8.58089745e-01]\n [ 6.22245148e-02 -1.28967035e+00  1.15861082e+00 -1.20361042e+00\n   1.31391525e+00  8.34747970e-01 -1.13384676e+00 -1.38861549e+00\n   9.28063095e-01  9.78892982e-01  4.70346242e-01  8.80061150e-01\n  -1.15648842e+00 -5.04126310e-01 -1.49836469e+00  1.06950259e+00]\n [ 3.28504682e-01 -6.39724970e-01 -1.41184616e+00 -4.57128316e-01\n  -9.04775739e-01  7.03065097e-01  5.17146409e-01  1.24215245e+00\n  -3.06963593e-01 -1.21161675e+00  4.20491040e-01  8.44523013e-01\n   7.60565996e-01  9.44122612e-01 -5.29612228e-02 -5.74826635e-02]\n [-3.18206370e-01 -1.68484068e+00 -1.25756729e+00 -1.33975708e+00\n   1.38768137e-01 -5.79373121e-01  3.18774097e-02  1.30288339e+00\n  -1.10905623e+00 -1.26965895e-01  5.54302752e-01 -9.58559096e-01\n  -1.14493942e+00 -7.45374858e-01 -1.32491231e+00  1.28999960e+00]\n [ 9.39090848e-01  3.95218790e-01  1.39964759e+00  1.09843326e+00\n  -1.36954582e+00  1.20632052e+00  1.47644922e-01  1.00412893e+00\n   1.21817541e+00 -4.71544147e-01 -1.48736143e+00 -9.61484075e-01\n   2.32666001e-01  1.10930169e+00  1.35040438e+00 -1.76525998e+00]\n [-4.33227643e-02 -3.43743563e-01 -8.60095143e-01 -1.06053889e+00\n  -8.63091767e-01  1.36182117e+00 -6.65505886e-01  1.42863214e-01\n   5.23486614e-01 -3.01357239e-01  1.23822451e+00  1.52802396e+00\n  -4.57165807e-01 -1.68746710e-02 -7.90785372e-01 -8.45156848e-01]\n [-1.60878682e+00  3.13658476e-01  1.16290532e-01 -1.27645493e+00\n  -1.06081378e+00  1.25482881e+00 -9.80176806e-01 -9.72758174e-01\n  -2.44940564e-01  2.01879978e+00 -1.06984508e+00  5.44060051e-01\n   1.54882884e+00 -9.28341269e-01  8.43595505e-01 -5.70529401e-01]\n [ 3.58262867e-01  3.95649731e-01  2.31463164e-01 -1.15391743e+00\n   8.05652916e-01 -5.59533298e-01 -1.17973459e+00 -1.28342819e+00\n   1.20048836e-01  8.69629264e-01 -1.78298116e+00  1.62234041e-03\n  -5.56657612e-01  5.02471805e-01 -1.27463794e+00  4.99452710e-01]\n [-4.53013629e-01  1.43296969e+00 -1.15445685e+00 -3.62144530e-01\n  -1.61463833e+00  1.30556357e+00  1.41924441e+00 -6.35451913e-01\n   3.68643790e-01  1.39055729e+00 -7.93898702e-02  6.11306690e-02\n  -4.96236235e-01 -1.43578756e+00  1.48994422e+00  1.19305325e+00]\n [ 3.60891342e-01 -6.11904263e-01 -4.17782009e-01  8.53060544e-01\n   1.01289427e+00  1.18941927e+00  1.05256987e+00  5.10586858e-01\n  -1.70328653e+00 -1.05482686e+00  1.00661039e+00  3.21358711e-01\n  -1.41163647e+00 -1.40640569e+00  5.96090555e-01 -1.77151966e+00]\n [-1.19939613e+00  1.05542779e-01  7.74761438e-01  6.19439125e-01\n  -1.24314022e+00  6.49244547e-01 -9.88877952e-01 -4.34172124e-01\n   6.79903805e-01  7.65443981e-01  8.50580871e-01  4.94837910e-01\n   7.88231611e-01 -1.43377936e+00 -5.35161257e-01 -9.10179675e-01]\n [-9.24594164e-01  1.55709398e+00 -2.65051574e-01  1.37745655e+00\n   1.21091820e-01  9.04441357e-01  9.55426414e-03  3.16260219e-01\n  -2.33912706e-01 -9.29444611e-01  4.49613094e-01 -7.82401145e-01\n  -1.35214984e+00  5.03523350e-01 -1.26414239e+00  1.32562959e+00]\n [ 1.42078876e+00  1.35816228e+00 -3.44879210e-01 -1.39018703e+00\n   1.11753559e+00 -2.27831841e-01  1.75526357e+00  1.47013462e+00\n   1.06316376e+00 -5.59403419e-01 -6.17412746e-01  1.15075493e+00\n  -2.00868532e-01 -1.16759050e+00  1.88007846e-01  1.31137955e+00]\n [ 5.68783939e-01  1.78508818e-01 -1.29485512e+00  5.02764702e-01\n   1.32453394e+00 -1.11758614e+00  6.85921833e-02  1.21279311e+00\n   6.59312785e-01  9.42183495e-01  3.86455745e-01 -5.15596747e-01\n  -2.92662948e-01  1.07891977e+00  1.15681589e+00  1.08264375e+00]\n [ 1.28637016e+00 -2.23820228e-02  1.12244241e-01  1.08145690e+00\n   1.84213027e-01  6.17705226e-01  1.11245263e+00  1.25048494e+00\n  -7.89896369e-01 -2.56770849e-01 -1.53818953e+00  2.25952566e-01\n  -1.30640531e+00 -1.27996191e-01  1.33864954e-01 -8.39525521e-01]\n [ 2.21252427e-01 -1.66745687e+00 -1.50305724e+00  1.15819585e+00\n  -7.87396252e-01 -1.15780723e+00  8.33158121e-02  8.92396629e-01\n  -1.22948813e+00  6.65693939e-01 -1.56549966e+00 -1.55886626e+00\n   6.42833352e-01  1.35451153e-01  4.96377140e-01  6.15848184e-01]\n [ 1.49321604e+00 -5.41970087e-03 -5.09142578e-01  1.07164097e+00\n  -1.08701479e+00 -1.94516957e-01 -1.58628154e+00 -1.32945120e+00\n   1.45765293e+00  1.46052504e+00  3.65865499e-01 -3.47954124e-01\n  -7.65982866e-01 -1.21342766e+00 -9.84443605e-01  3.02398372e-02]\n [ 6.30119741e-01  4.86886114e-01 -6.58860803e-01  1.57579315e+00\n   4.79052186e-01  1.61825076e-01  4.19944465e-01 -1.53099254e-01\n  -1.11467373e+00 -3.29917789e-01  5.61561584e-01 -1.68524861e+00\n  -9.91126060e-01 -1.60115123e+00 -1.78574336e+00  1.04419661e+00]\n [ 5.93984365e-01 -1.49544507e-01 -1.29256654e+00  1.13183148e-01\n  -4.07564849e-01 -1.01530647e+00 -2.49227628e-01 -2.16043025e-01\n   2.09846586e-01  7.11212158e-01 -1.69215405e+00 -4.64345008e-01\n   1.01467180e+00  3.79660563e-03  1.33418548e+00  3.92690778e-01]\n [-1.19237137e+00 -1.53037310e+00  6.02585554e-01 -1.35528433e+00\n  -3.10103670e-02  1.35354602e+00  2.83579081e-01 -2.46879801e-01\n   3.08570981e-01  5.15905917e-02 -1.09703377e-01  1.45690715e+00\n   7.13280514e-02  1.61197674e+00  1.52105665e+00 -1.14000404e+00]\n [-1.50150323e+00 -1.42702007e+00 -1.56961691e+00 -1.14080465e+00\n   2.95005649e-01 -1.33035910e+00 -6.81409657e-01  1.11582708e+00\n  -1.92229426e+00  1.38028586e+00 -9.43962336e-01 -1.33353257e+00\n   1.29354393e+00  4.45490211e-01  1.41443312e+00  6.45580411e-01]\n [ 9.23763812e-01 -8.06898475e-01 -1.01554036e+00  9.30916309e-01\n   7.10200608e-01  1.50881255e+00 -3.29113245e-01 -2.95073271e-01\n   7.87563801e-01 -3.86499196e-01  1.10846639e+00  1.17541599e+00\n   2.40087569e-01  8.73567283e-01  9.44282949e-01 -1.44683027e+00]\n [ 1.25106192e+00 -4.32174429e-02  1.24303710e+00 -4.28500354e-01\n   1.00757301e+00 -3.48223358e-01 -1.84067416e+00  1.29636538e+00\n  -1.67757213e+00 -4.66657221e-01  1.16952550e+00  1.48789370e+00\n   8.08413208e-01  4.55652028e-01 -2.26405397e-01 -8.17442417e-01]\n [-6.44858658e-01  5.29039443e-01  9.85229015e-01  1.06025207e+00\n   6.52473330e-01 -1.26853812e+00 -2.13586148e-02 -1.23334968e+00\n  -2.87821610e-02 -1.07844472e-02  9.72292602e-01 -5.44664145e-01\n  -9.87213492e-01 -1.26063287e+00  9.70931590e-01  2.58674026e-01]\n [-1.39657509e+00 -1.48405612e+00  8.06338608e-01 -1.20925438e+00\n   7.60580420e-01  6.30909026e-01 -1.57539988e+00 -1.15195525e+00\n   1.54397500e+00 -2.61665255e-01 -6.63134634e-01  1.02081764e+00\n   2.27920032e+00  1.69908381e+00  9.39828515e-01 -5.42463362e-01]\n [-1.45479167e+00  8.86998117e-01  3.10213894e-01 -9.95984375e-02\n   1.04388976e+00 -1.20679784e+00 -2.81124450e-02 -1.37121546e+00\n  -3.19752187e-01 -1.44769406e+00 -1.45963526e+00 -1.33569694e+00\n   1.10654569e+00  8.56623054e-01  2.89616942e-01  1.39752567e+00]\n [-4.92210567e-01 -7.94316828e-01  1.38968980e+00 -7.33032286e-01\n   1.23456860e+00 -1.51267731e+00  1.76739264e+00 -1.27631259e+00\n   1.20037150e+00  3.10634613e-01  1.30522394e+00 -1.48391163e+00\n   7.31360018e-01  1.64045715e+00  5.91070466e-02  2.95693517e-01]\n [ 5.67855239e-01 -2.16712832e-01  5.50868690e-01  4.05858457e-01\n   1.02646625e+00 -1.40986013e+00 -8.24418545e-01  1.43072331e+00\n   1.19720757e+00  4.19069082e-02  1.25984386e-01 -7.93894947e-01\n  -7.07645535e-01 -1.34665519e-01 -5.90095639e-01  1.44237578e-01]\n [-1.47384083e+00  1.56182992e+00  1.79897726e+00  7.65307069e-01\n  -1.97584689e-01 -5.94284832e-01  1.18018043e+00  6.37992740e-01\n  -1.42092049e+00  1.74007988e+00  7.66174614e-01  1.48515773e+00\n   1.40757704e+00  3.90974283e-01 -3.41916680e-01  1.30003476e+00]]\\  actual=[[ 72.12897     37.10405     15.7523     -55.13009    -55.133953\n  -70.81297     58.5914      16.145678    33.273388   -76.81792\n   75.20286     53.189465   -46.11118    -50.997578   -50.744637\n  -31.394217  ]\n [  3.9179523  -10.944458   -33.47808     17.865194   -57.776295\n  -33.33147    -21.446657    -7.0811925   45.620388   -48.13935\n    2.2330067   14.752427   -72.67586     17.175323   -52.807247\n  -69.69715   ]\n [ 71.83611     74.51782     49.338955   -31.334723   -64.47344\n   29.45583     -9.630149   -60.57156     -0.81878185 -74.6074\n   65.50033    -38.674343    25.979166   -30.19819      3.1671708\n    7.433544  ]\n [-50.512447    75.15077     44.012123    70.332985    63.179474\n   15.6308365   67.51064    -65.94343    -48.730396   -72.87173\n  -28.017263   -17.873161   -36.66159     52.596138   -22.985323\n  -35.126614  ]\n [  6.7907286  -57.547245    48.346058   -68.17602     77.921486\n   43.54964    -48.292774   -79.22994     50.470165    33.07879\n   36.625763    43.3936     -68.25704    -22.711107   -61.559464\n   58.099346  ]\n [ 19.697985   -27.125677   -69.93627    -30.31489    -28.040804\n   36.721684    21.981413    61.960106    -4.495813   -60.962933\n   34.101646    41.714535     9.766227    43.345055    -1.0399792\n    3.5939016 ]\n [-11.649695   -76.04372    -62.83697    -75.08129     21.797728\n  -29.774647     1.3260391   65.21946    -40.19367    -14.397317\n   40.8764     -43.4755     -67.787      -33.714706   -54.29696\n   68.76345   ]\n [ 49.2946      21.316254    59.437622    48.58227    -50.23772\n   62.81623      6.2536592   49.185677    63.381878   -29.190554\n  -62.490997   -43.613693   -11.71907     50.879047    57.719368\n  -79.00094   ]\n [  1.674592   -13.271871   -44.54686    -60.919514   -26.050024\n   70.87917    -28.357935     2.9626138   32.464127   -21.884188\n   75.50266     74.007835   -39.79492     -0.48704565 -31.932901\n  -34.50113   ]\n [-74.20731     17.498714     0.38257343 -71.87064    -35.49301\n   65.33146    -41.751842   -56.911404    -1.735422    77.72348\n  -41.352562    27.518593    41.848175   -42.059998    36.499123\n  -21.219059  ]\n [ 21.140436    21.336424     5.6823664  -65.65562     53.647427\n  -28.745922   -50.245975   -73.58467     14.50876     28.38794\n  -77.457924     1.8900954  -43.844193    23.200895   -52.191956\n   30.529491  ]\n [-18.184135    69.88958    -58.09223    -25.497366   -61.943077\n   67.96213     60.379025   -38.808594    25.572704    50.75213\n    8.79318      4.7016788  -41.38507    -65.205154    63.561943\n   64.074745  ]\n [ 21.267845   -25.823492   -24.193348    36.13711     63.54505\n   61.939877    44.7716      22.697887   -66.64043    -54.231716\n   63.77627     16.996647   -78.64145    -63.86502     26.136017\n  -79.30369   ]\n [-54.363132     7.7575803   30.682816    24.287971   -44.200726\n   33.93101    -42.122204   -28.006155    39.425613    23.915117\n   55.87666     25.192999    10.892204   -65.11356    -21.229841\n  -37.64589   ]\n [-41.042805    75.69939    -17.165293    62.73417     20.953526\n   47.163345     0.37585893  12.268615    -1.244617   -48.848877\n   35.576065   -35.15258    -76.220375    23.248857   -51.752506\n   70.48666   ]\n [ 72.643684    66.38812    -20.838646   -77.63907     68.54261\n  -11.546707    74.68161     74.195625    56.482944   -32.96248\n  -18.446375    56.18304    -29.363735   -52.972404     9.049469\n   69.79747   ]\n [ 31.344912    11.1728525  -64.55281     18.370316    78.42863\n  -57.681793     2.888796    60.384426    38.50919     31.502796\n   32.378475   -22.546902   -33.099728    49.493294    49.613758\n   58.734905  ]\n [ 66.12809      1.7698882    0.19637921  47.72124     23.968128\n   32.29565     47.3205      62.4073     -25.989174   -19.97003\n  -65.06438     12.489002   -74.35859     -5.555415     6.78249\n  -34.228775  ]\n [ 14.499204   -75.23005    -74.13345     51.613396   -22.434887\n  -59.76732      3.515504    43.18914    -45.5536      19.632704\n  -66.44706    -71.83814      4.974559     6.4606953   21.960985\n   36.158836  ]\n [ 76.15441      2.5638325  -28.397402    47.223377   -36.744347\n   -9.819282   -67.55057    -76.054665    74.04004     53.75594\n   31.33601    -14.626307   -52.363636   -55.063087   -40.04143\n    7.8365073 ]\n [ 34.31801     25.606867   -35.286846    72.79368     38.049328\n    8.657584    17.844032   -12.921325   -40.44368    -23.110332\n   41.243904   -77.80932    -61.526867   -72.74757    -73.592094\n   56.875454  ]\n [ 32.566437    -4.1821227  -64.447495    -1.3890303   -4.2945476\n  -52.378445   -10.639139   -16.299437    18.505291    21.586868\n  -72.85944    -20.125416    20.108217     0.45579308  57.040276\n   25.366064  ]\n [-54.022625   -68.81366     22.75996    -75.86883     13.68929\n   70.45009     12.039668   -17.954409    22.899109    -6.7316313\n    7.2584376   70.64779    -18.285454    73.80659     64.864624\n  -48.7611    ]\n [-69.00701    -63.97608    -77.196266   -64.99055     29.259462\n  -68.71439    -29.034878    55.180374   -76.38756     50.31116\n  -34.979248   -61.191833    31.458187    20.601904    60.40027\n   37.596806  ]\n [ 48.551662   -34.950455   -51.69984     40.085907    49.088726\n   78.50089    -14.039453   -20.540892    44.217117   -25.539452\n   68.93314     57.3482     -11.417016    40.12695     40.71494\n  -63.60042   ]\n [ 64.41661      0.7946582   52.231026   -28.862896    63.290913\n  -17.789188   -78.37873     64.869644   -65.49599    -28.980751\n   72.024506    72.11181     11.713585    21.065395    -8.30214\n  -33.160748  ]\n [-27.483337    27.57991     40.36771     46.645744    46.331734\n  -65.50888     -0.93994135 -70.89702      7.884903    -9.409481\n   62.038803   -23.920244   -61.36763    -57.216167    41.830723\n   18.884485  ]\n [-63.920887   -66.64573     32.135876   -68.46228     51.494812\n   32.980286   -67.08739    -66.52869     77.88188    -20.180153\n  -20.761229    50.043907    71.573975    77.77963     40.528427\n  -19.861677  ]\n [-66.74279     44.33465      9.306165   -12.181187    65.02536\n  -62.307552    -1.2274168  -78.296104    -5.0649805  -71.09807\n  -61.08725    -61.29409     23.847445    39.354107    13.303872\n   73.96384   ]\n [-20.084106   -34.361557    58.979404   -44.308575    74.13198\n  -78.16786     75.19788    -73.20279     62.589497     4.3894997\n   78.89477    -68.29677      8.577554    75.105606     3.6523533\n   20.674894  ]\n [ 31.299896    -7.326027    20.380154    13.455288    64.19324\n  -72.83664    -35.12202     72.08047     62.448685    -7.1473656\n   19.19107    -35.69563    -49.989334    -5.8596106  -23.52996\n   13.349887  ]\n [-67.666145    75.92105     77.8132      31.686308     5.733879\n  -30.54784     50.203316    29.5356     -54.073467    65.75764\n   51.603256    71.98254     36.09929     18.115376   -13.138633\n   69.248795  ]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-25c63c577cf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0massertAlmostEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_layer_output_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_layer_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtest_BatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-25c63c577cf8>\u001b[0m in \u001b[0;36mtest_BatchNormalization\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlayer_input_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtorch_layer_output_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_input_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0massertAlmostEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_layer_output_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_layer_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# 2. check layer input grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f7a9542a9d61>\u001b[0m in \u001b[0;36massertAlmostEqual\u001b[0;34m(expected, actual, msg, rtol, atol)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{msg}: FAILED:\\n  expected={expected}\\n  actual={actual}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{msg}: FAILED:\\n  expected={expected}\\  actual={actual}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0massertTrue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"???\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: ???: FAILED:\n  expected=[[ 1.41016996e+00  7.32519150e-01  4.50298339e-01 -9.46392715e-01\n  -1.47206545e+00 -1.37083197e+00  1.37724662e+00  3.88500780e-01\n   5.41669905e-01 -1.58092630e+00  1.23230290e+00  1.08739460e+00\n  -6.12358093e-01 -1.12429333e+00 -1.24007118e+00 -7.80916631e-01]\n [ 2.95836106e-03 -2.94019282e-01 -6.19553506e-01  4.92805600e-01\n  -1.52739215e+00 -6.47969484e-01 -5.03135085e-01 -4.42811698e-02\n   8.19093764e-01 -9.12917614e-01 -2.08963335e-01  2.73858875e-01\n  -1.26506019e+00  3.70364130e-01 -1.28933299e+00 -1.57288957e+00]\n [ 1.40412831e+00  1.53185034e+00  1.18018794e+00 -4.77235675e-01\n  -1.66762042e+00  5.62936842e-01 -2.25522816e-01 -1.04095721e+00\n  -2.24344641e-01 -1.52943671e+00  1.04066312e+00 -8.56940687e-01\n   1.15892267e+00 -6.68276966e-01  4.75193001e-02  2.19079480e-02]\n [-1.11995554e+00  1.54537308e+00  1.06442773e+00  1.52727723e+00\n   1.00523961e+00  2.96310157e-01  1.58679175e+00 -1.14105034e+00\n  -1.30086732e+00 -1.48900783e+00 -8.06452513e-01 -4.16675121e-01\n  -3.80178988e-01  1.14694810e+00 -5.77087879e-01 -8.58089745e-01]\n [ 6.22245148e-02 -1.28967035e+00  1.15861082e+00 -1.20361042e+00\n   1.31391525e+00  8.34747970e-01 -1.13384676e+00 -1.38861549e+00\n   9.28063095e-01  9.78892982e-01  4.70346242e-01  8.80061150e-01\n  -1.15648842e+00 -5.04126310e-01 -1.49836469e+00  1.06950259e+00]\n [ 3.28504682e-01 -6.39724970e-01 -1.41184616e+00 -4.57128316e-01\n  -9.04775739e-01  7.03065097e-01  5.17146409e-01  1.24215245e+00\n  -3.06963593e-01 -1.21161675e+00  4.20491040e-01  8.44523013e-01\n   7.60565996e-01  9.44122612e-01 -5.29612228e-02 -5.74826635e-02]\n [-3.18206370e-01 -1.68484068e+00 -1.25756729e+00 -1.33975708e+00\n   1.38768137e-01 -5.79373121e-01  3.18774097e-02  1.30288339e+00\n  -1.10905623e+00 -1.26965895e-01  5.54302752e-01 -9.58559096e-01\n  -1.14493942e+00 -7.45374858e-01 -1.32491231e+00  1.28999960e+00]\n [ 9.39090848e-01  3.95218790e-01  1.39964759e+00  1.09843326e+00\n  -1.36954582e+00  1.20632052e+00  1.47644922e-01  1.00412893e+00\n   1.21817541e+00 -4.71544147e-01 -1.48736143e+00 -9.61484075e-01\n   2.32666001e-01  1.10930169e+00  1.35040438e+00 -1.76525998e+00]\n [-4.33227643e-02 -3.43743563e-01 -8.60095143e-01 -1.06053889e+00\n  -8.63091767e-01  1.36182117e+00 -6.65505886e-01  1.42863214e-01\n   5.23486614e-01 -3.01357239e-01  1.23822451e+00  1.52802396e+00\n  -4.57165807e-01 -1.68746710e-02 -7.90785372e-01 -8.45156848e-01]\n [-1.60878682e+00  3.13658476e-01  1.16290532e-01 -1.27645493e+00\n  -1.06081378e+00  1.25482881e+00 -9.80176806e-01 -9.72758174e-01\n  -2.44940564e-01  2.01879978e+00 -1.06984508e+00  5.44060051e-01\n   1.54882884e+00 -9.28341269e-01  8.43595505e-01 -5.70529401e-01]\n [ 3.58262867e-01  3.95649731e-01  2.31463164e-01 -1.15391743e+00\n   8.05652916e-01 -5.59533298e-01 -1.17973459e+00 -1.28342819e+00\n   1.20048836e-01  8.69629264e-01 -1.78298116e+00  1.62234041e-03\n  -5.56657612e-01  5.02471805e-01 -1.27463794e+00  4.99452710e-01]\n [-4.53013629e-01  1.43296969e+00 -1.15445685e+00 -3.62144530e-01\n  -1.61463833e+00  1.30556357e+00  1.41924441e+00 -6.35451913e-01\n   3.68643790e-01  1.39055729e+00 -7.93898702e-02  6.11306690e-02\n  -4.96236235e-01 -1.43578756e+00  1.48994422e+00  1.19305325e+00]\n [ 3.60891342e-01 -6.11904263e-01 -4.17782009e-01  8.53060544e-01\n   1.01289427e+00  1.18941927e+00  1.05256987e+00  5.10586858e-01\n  -1.70328653e+00 -1.05482686e+00  1.00661039e+00  3.21358711e-01\n  -1.41163647e+00 -1.40640569e+00  5.96090555e-01 -1.77151966e+00]\n [-1.19939613e+00  1.05542779e-01  7.74761438e-01  6.19439125e-01\n  -1.24314022e+00  6.49244547e-01 -9.88877952e-01 -4.34172124e-01\n   6.79903805e-01  7.65443981e-01  8.50580871e-01  4.94837910e-01\n   7.88231611e-01 -1.43377936e+00 -5.35161257e-01 -9.10179675e-01]\n [-9.24594164e-01  1.55709398e+00 -2.65051574e-01  1.37745655e+00\n   1.21091820e-01  9.04441357e-01  9.55426414e-03  3.16260219e-01\n  -2.33912706e-01 -9.29444611e-01  4.49613094e-01 -7.82401145e-01\n  -1.35214984e+00  5.03523350e-01 -1.26414239e+00  1.32562959e+00]\n [ 1.42078876e+00  1.35816228e+00 -3.44879210e-01 -1.39018703e+00\n   1.11753559e+00 -2.27831841e-01  1.75526357e+00  1.47013462e+00\n   1.06316376e+00 -5.59403419e-01 -6.17412746e-01  1.15075493e+00\n  -2.00868532e-01 -1.16759050e+00  1.88007846e-01  1.31137955e+00]\n [ 5.68783939e-01  1.78508818e-01 -1.29485512e+00  5.02764702e-01\n   1.32453394e+00 -1.11758614e+00  6.85921833e-02  1.21279311e+00\n   6.59312785e-01  9.42183495e-01  3.86455745e-01 -5.15596747e-01\n  -2.92662948e-01  1.07891977e+00  1.15681589e+00  1.08264375e+00]\n [ 1.28637016e+00 -2.23820228e-02  1.12244241e-01  1.08145690e+00\n   1.84213027e-01  6.17705226e-01  1.11245263e+00  1.25048494e+00\n  -7.89896369e-01 -2.56770849e-01 -1.53818953e+00  2.25952566e-01\n  -1.30640531e+00 -1.27996191e-01  1.33864954e-01 -8.39525521e-01]\n [ 2.21252427e-01 -1.66745687e+00 -1.50305724e+00  1.15819585e+00\n  -7.87396252e-01 -1.15780723e+00  8.33158121e-02  8.92396629e-01\n  -1.22948813e+00  6.65693939e-01 -1.56549966e+00 -1.55886626e+00\n   6.42833352e-01  1.35451153e-01  4.96377140e-01  6.15848184e-01]\n [ 1.49321604e+00 -5.41970087e-03 -5.09142578e-01  1.07164097e+00\n  -1.08701479e+00 -1.94516957e-01 -1.58628154e+00 -1.32945120e+00\n   1.45765293e+00  1.46052504e+00  3.65865499e-01 -3.47954124e-01\n  -7.65982866e-01 -1.21342766e+00 -9.84443605e-01  3.02398372e-02]\n [ 6.30119741e-01  4.86886114e-01 -6.58860803e-01  1.57579315e+00\n   4.79052186e-01  1.61825076e-01  4.19944465e-01 -1.53099254e-01\n  -1.11467373e+00 -3.29917789e-01  5.61561584e-01 -1.68524861e+00\n  -9.91126060e-01 -1.60115123e+00 -1.78574336e+00  1.04419661e+00]\n [ 5.93984365e-01 -1.49544507e-01 -1.29256654e+00  1.13183148e-01\n  -4.07564849e-01 -1.01530647e+00 -2.49227628e-01 -2.16043025e-01\n   2.09846586e-01  7.11212158e-01 -1.69215405e+00 -4.64345008e-01\n   1.01467180e+00  3.79660563e-03  1.33418548e+00  3.92690778e-01]\n [-1.19237137e+00 -1.53037310e+00  6.02585554e-01 -1.35528433e+00\n  -3.10103670e-02  1.35354602e+00  2.83579081e-01 -2.46879801e-01\n   3.08570981e-01  5.15905917e-02 -1.09703377e-01  1.45690715e+00\n   7.13280514e-02  1.61197674e+00  1.52105665e+00 -1.14000404e+00]\n [-1.50150323e+00 -1.42702007e+00 -1.56961691e+00 -1.14080465e+00\n   2.95005649e-01 -1.33035910e+00 -6.81409657e-01  1.11582708e+00\n  -1.92229426e+00  1.38028586e+00 -9.43962336e-01 -1.33353257e+00\n   1.29354393e+00  4.45490211e-01  1.41443312e+00  6.45580411e-01]\n [ 9.23763812e-01 -8.06898475e-01 -1.01554036e+00  9.30916309e-01\n   7.10200608e-01  1.50881255e+00 -3.29113245e-01 -2.95073271e-01\n   7.87563801e-01 -3.86499196e-01  1.10846639e+00  1.17541599e+00\n   2.40087569e-01  8.73567283e-01  9.44282949e-01 -1.44683027e+00]\n [ 1.25106192e+00 -4.32174429e-02  1.24303710e+00 -4.28500354e-01\n   1.00757301e+00 -3.48223358e-01 -1.84067416e+00  1.29636538e+00\n  -1.67757213e+00 -4.66657221e-01  1.16952550e+00  1.48789370e+00\n   8.08413208e-01  4.55652028e-01 -2.26405397e-01 -8.17442417e-01]\n [-6.44858658e-01  5.29039443e-01  9.85229015e-01  1.06025207e+00\n   6.52473330e-01 -1.26853812e+00 -2.13586148e-02 -1.23334968e+00\n  -2.87821610e-02 -1.07844472e-02  9.72292602e-01 -5.44664145e-01\n  -9.87213492e-01 -1.26063287e+00  9.70931590e-01  2.58674026e-01]\n [-1.39657509e+00 -1.48405612e+00  8.06338608e-01 -1.20925438e+00\n   7.60580420e-01  6.30909026e-01 -1.57539988e+00 -1.15195525e+00\n   1.54397500e+00 -2.61665255e-01 -6.63134634e-01  1.02081764e+00\n   2.27920032e+00  1.69908381e+00  9.39828515e-01 -5.42463362e-01]\n [-1.45479167e+00  8.86998117e-01  3.10213894e-01 -9.95984375e-02\n   1.04388976e+00 -1.20679784e+00 -2.81124450e-02 -1.37121546e+00\n  -3.19752187e-01 -1.44769406e+00 -1.45963526e+00 -1.33569694e+00\n   1.10654569e+00  8.56623054e-01  2.89616942e-01  1.39752567e+00]\n [-4.92210567e-01 -7.94316828e-01  1.38968980e+00 -7.33032286e-01\n   1.23456860e+00 -1.51267731e+00  1.76739264e+00 -1.27631259e+00\n   1.20037150e+00  3.10634613e-01  1.30522394e+00 -1.48391163e+00\n   7.31360018e-01  1.64045715e+00  5.91070466e-02  2.95693517e-01]\n [ 5.67855239e-01 -2.16712832e-01  5.50868690e-01  4.05858457e-01\n   1.02646625e+00 -1.40986013e+00 -8.24418545e-01  1.43072331e+00\n   1.19720757e+00  4.19069082e-02  1.25984386e-01 -7.93894947e-01\n  -7.07645535e-01 -1.34665519e-01 -5.90095639e-01  1.44237578e-01]\n [-1.47384083e+00  1.56182992e+00  1.79897726e+00  7.65307069e-01\n  -1.97584689e-01 -5.94284832e-01  1.18018043e+00  6.37992740e-01\n  -1.42092049e+00  1.74007988e+00  7.66174614e-01  1.48515773e+00\n   1.40757704e+00  3.90974283e-01 -3.41916680e-01  1.30003476e+00]]\\  actual=[[ 72.12897     37.10405     15.7523     -55.13009    -55.133953\n  -70.81297     58.5914      16.145678    33.273388   -76.81792\n   75.20286     53.189465   -46.11118    -50.997578   -50.744637\n  -31.394217  ]\n [  3.9179523  -10.944458   -33.47808     17.865194   -57.776295\n  -33.33147    -21.446657    -7.0811925   45.620388   -48.13935\n    2.2330067   14.752427   -72.67586     17.175323   -52.807247\n  -69.69715   ]\n [ 71.83611     74.51782     49.338955   -31.334723   -64.47344\n   29.45583     -9.630149   -60.57156     -0.81878185 -74.6074\n   65.50033    -38.674343    25.979166   -30.19819      3.1671708\n    7.433544  ]\n [-50.512447    75.15077     44.012123    70.332985    63.179474\n   15.6308365   67.51064    -65.94343    -48.730396   -72.87173\n  -28.017263   -17.873161   -36.66159     52.596138   -22.985323\n  -35.126614  ]\n [  6.7907286  -57.547245    48.346058   -68.17602     77.921486\n   43.54964    -48.292774   -79.22994     50.470165    33.07879\n   36.625763    43.3936     -68.25704    -22.711107   -61.559464\n   58.099346  ]\n [ 19.697985   -27.125677   -69.93627    -30.31489    -28.040804\n   36.721684    21.981413    61.960106    -4.495813   -60.962933\n   34.101646    41.714535     9.766227    43.345055    -1.0399792\n    3.5939016 ]\n [-11.649695   -76.04372    -62.83697    -75.08129     21.797728\n  -29.774647     1.3260391   65.21946    -40.19367    -14.397317\n   40.8764     -43.4755     -67.787      -33.714706   -54.29696\n   68.76345   ]\n [ 49.2946      21.316254    59.437622    48.58227    -50.23772\n   62.81623      6.2536592   49.185677    63.381878   -29.190554\n  -62.490997   -43.613693   -11.71907     50.879047    57.719368\n  -79.00094   ]\n [  1.674592   -13.271871   -44.54686    -60.919514   -26.050024\n   70.87917    -28.357935     2.9626138   32.464127   -21.884188\n   75.50266     74.007835   -39.79492     -0.48704565 -31.932901\n  -34.50113   ]\n [-74.20731     17.498714     0.38257343 -71.87064    -35.49301\n   65.33146    -41.751842   -56.911404    -1.735422    77.72348\n  -41.352562    27.518593    41.848175   -42.059998    36.499123\n  -21.219059  ]\n [ 21.140436    21.336424     5.6823664  -65.65562     53.647427\n  -28.745922   -50.245975   -73.58467     14.50876     28.38794\n  -77.457924     1.8900954  -43.844193    23.200895   -52.191956\n   30.529491  ]\n [-18.184135    69.88958    -58.09223    -25.497366   -61.943077\n   67.96213     60.379025   -38.808594    25.572704    50.75213\n    8.79318      4.7016788  -41.38507    -65.205154    63.561943\n   64.074745  ]\n [ 21.267845   -25.823492   -24.193348    36.13711     63.54505\n   61.939877    44.7716      22.697887   -66.64043    -54.231716\n   63.77627     16.996647   -78.64145    -63.86502     26.136017\n  -79.30369   ]\n [-54.363132     7.7575803   30.682816    24.287971   -44.200726\n   33.93101    -42.122204   -28.006155    39.425613    23.915117\n   55.87666     25.192999    10.892204   -65.11356    -21.229841\n  -37.64589   ]\n [-41.042805    75.69939    -17.165293    62.73417     20.953526\n   47.163345     0.37585893  12.268615    -1.244617   -48.848877\n   35.576065   -35.15258    -76.220375    23.248857   -51.752506\n   70.48666   ]\n [ 72.643684    66.38812    -20.838646   -77.63907     68.54261\n  -11.546707    74.68161     74.195625    56.482944   -32.96248\n  -18.446375    56.18304    -29.363735   -52.972404     9.049469\n   69.79747   ]\n [ 31.344912    11.1728525  -64.55281     18.370316    78.42863\n  -57.681793     2.888796    60.384426    38.50919     31.502796\n   32.378475   -22.546902   -33.099728    49.493294    49.613758\n   58.734905  ]\n [ 66.12809      1.7698882    0.19637921  47.72124     23.968128\n   32.29565     47.3205      62.4073     -25.989174   -19.97003\n  -65.06438     12.489002   -74.35859     -5.555415     6.78249\n  -34.228775  ]\n [ 14.499204   -75.23005    -74.13345     51.613396   -22.434887\n  -59.76732      3.515504    43.18914    -45.5536      19.632704\n  -66.44706    -71.83814      4.974559     6.4606953   21.960985\n   36.158836  ]\n [ 76.15441      2.5638325  -28.397402    47.223377   -36.744347\n   -9.819282   -67.55057    -76.054665    74.04004     53.75594\n   31.33601    -14.626307   -52.363636   -55.063087   -40.04143\n    7.8365073 ]\n [ 34.31801     25.606867   -35.286846    72.79368     38.049328\n    8.657584    17.844032   -12.921325   -40.44368    -23.110332\n   41.243904   -77.80932    -61.526867   -72.74757    -73.592094\n   56.875454  ]\n [ 32.566437    -4.1821227  -64.447495    -1.3890303   -4.2945476\n  -52.378445   -10.639139   -16.299437    18.505291    21.586868\n  -72.85944    -20.125416    20.108217     0.45579308  57.040276\n   25.366064  ]\n [-54.022625   -68.81366     22.75996    -75.86883     13.68929\n   70.45009     12.039668   -17.954409    22.899109    -6.7316313\n    7.2584376   70.64779    -18.285454    73.80659     64.864624\n  -48.7611    ]\n [-69.00701    -63.97608    -77.196266   -64.99055     29.259462\n  -68.71439    -29.034878    55.180374   -76.38756     50.31116\n  -34.979248   -61.191833    31.458187    20.601904    60.40027\n   37.596806  ]\n [ 48.551662   -34.950455   -51.69984     40.085907    49.088726\n   78.50089    -14.039453   -20.540892    44.217117   -25.539452\n   68.93314     57.3482     -11.417016    40.12695     40.71494\n  -63.60042   ]\n [ 64.41661      0.7946582   52.231026   -28.862896    63.290913\n  -17.789188   -78.37873     64.869644   -65.49599    -28.980751\n   72.024506    72.11181     11.713585    21.065395    -8.30214\n  -33.160748  ]\n [-27.483337    27.57991     40.36771     46.645744    46.331734\n  -65.50888     -0.93994135 -70.89702      7.884903    -9.409481\n   62.038803   -23.920244   -61.36763    -57.216167    41.830723\n   18.884485  ]\n [-63.920887   -66.64573     32.135876   -68.46228     51.494812\n   32.980286   -67.08739    -66.52869     77.88188    -20.180153\n  -20.761229    50.043907    71.573975    77.77963     40.528427\n  -19.861677  ]\n [-66.74279     44.33465      9.306165   -12.181187    65.02536\n  -62.307552    -1.2274168  -78.296104    -5.0649805  -71.09807\n  -61.08725    -61.29409     23.847445    39.354107    13.303872\n   73.96384   ]\n [-20.084106   -34.361557    58.979404   -44.308575    74.13198\n  -78.16786     75.19788    -73.20279     62.589497     4.3894997\n   78.89477    -68.29677      8.577554    75.105606     3.6523533\n   20.674894  ]\n [ 31.299896    -7.326027    20.380154    13.455288    64.19324\n  -72.83664    -35.12202     72.08047     62.448685    -7.1473656\n   19.19107    -35.69563    -49.989334    -5.8596106  -23.52996\n   13.349887  ]\n [-67.666145    75.92105     77.8132      31.686308     5.733879\n  -30.54784     50.203316    29.5356     -54.073467    65.75764\n   51.603256    71.98254     36.09929     18.115376   -13.138633\n   69.248795  ]]"
     ]
    }
   ],
   "source": [
    "def test_BatchNormalization():\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    batch_size, n_in = 32, 16\n",
    "    for _ in range(100):\n",
    "        # layers initialization\n",
    "        slope = np.random.uniform(0.01, 0.05)\n",
    "        alpha = 0.9\n",
    "        custom_layer = BatchNormalization(alpha)\n",
    "        custom_layer.train()\n",
    "        torch_layer = torch.nn.BatchNorm1d(n_in, eps=custom_layer.EPS, momentum=1.-alpha, affine=False)\n",
    "        custom_layer.moving_mean = torch_layer.running_mean.numpy().copy()\n",
    "        custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
    "\n",
    "        layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "        next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "        # 1. check layer output\n",
    "        custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "        torch_layer_output_var = torch_layer(layer_input_var)\n",
    "        assertAlmostEqual(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6)\n",
    "\n",
    "        # 2. check layer input grad\n",
    "        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "        torch_layer_grad_var = layer_input_var.grad\n",
    "        # please, don't increase `atol` parameter, it's garanteed that you can implement batch norm layer\n",
    "        # with tolerance 1e-5\n",
    "        assertAlmostEqual(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-5)\n",
    "\n",
    "        # 3. check moving mean\n",
    "        assertAlmostEqual(custom_layer.moving_mean, torch_layer.running_mean.numpy())\n",
    "        # we don't check moving_variance because pytorch uses slightly different formula for it:\n",
    "        # it computes moving average for unbiased variance (i.e var*N/(N-1))\n",
    "        #self.assertTrue(np.allclose(custom_layer.moving_variance, torch_layer.running_var.numpy()))\n",
    "\n",
    "        # 4. check evaluation mode\n",
    "        custom_layer.moving_variance = torch_layer.running_var.numpy().copy()\n",
    "        custom_layer.evaluate()\n",
    "        custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "        torch_layer.eval()\n",
    "        torch_layer_output_var = torch_layer(layer_input_var)\n",
    "        assertAlmostEqual(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6)\n",
    "            \n",
    "test_BatchNormalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelwiseScaling(Module):\n",
    "    \"\"\"\n",
    "       Implements linear transform of input y = \\gamma * x + \\beta\n",
    "       where \\gamma, \\beta - learnable vectors of length x.shape[-1]\n",
    "    \"\"\"\n",
    "    def __init__(self, n_out):\n",
    "        super(ChannelwiseScaling, self).__init__()\n",
    "\n",
    "        stdv = 1./np.sqrt(n_out)\n",
    "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "        \n",
    "        self.gradGamma = np.zeros_like(self.gamma)\n",
    "        self.gradBeta = np.zeros_like(self.beta)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        self.output = input * self.gamma + self.beta\n",
    "        return self.output\n",
    "        \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = gradOutput * self.gamma\n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        self.gradBeta = np.sum(gradOutput, axis=0)\n",
    "        self.gradGamma = np.sum(gradOutput*input, axis=0)\n",
    "    \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradGamma.fill(0)\n",
    "        self.gradBeta.fill(0)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [self.gradGamma, self.gradBeta]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ChannelwiseScaling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical notes. If BatchNormalization is placed after a linear transformation layer (including dense layer, convolutions, channelwise scaling) that implements function like `y = weight * x + bias`, than bias adding become useless and could be omitted since its effect will be discarded while batch mean subtraction. If BatchNormalization (followed by `ChannelwiseScaling`) is placed before a layer that propagates scale (including ReLU, LeakyReLU) followed by any linear transformation layer than parameter `gamma` in `ChannelwiseScaling` could be freezed since it could be absorbed into the linear transformation layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dropout\n",
    "Implement [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. Here $p$ is probability of an element to be zeroed.\n",
    "\n",
    "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons.\n",
    "\n",
    "While training (`self.training == True`) it should sample a mask on each iteration (for every batch), zero out elements and multiply elements by $1 / (1 - p)$. The latter is needed for keeping mean values of features close to mean values which will be in test mode. When testing this module should implement identity transform i.e. `self.output = input`.\n",
    "\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- output: **`batch_size x n_feats`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(Dropout, self).__init__()\n",
    "        \n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        \n",
    "        if(self.training):\n",
    "            #\n",
    "        else:\n",
    "            self.output = input\n",
    "        return  self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        if(self.training):\n",
    "            #\n",
    "        else:\n",
    "            self.gradInput = 1 * gradOutput\n",
    "        return self.gradInput\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Dropout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Dropout():\n",
    "    np.random.seed(42)\n",
    "\n",
    "    batch_size, n_in = 2, 4\n",
    "    for _ in range(100):\n",
    "        # layers initialization\n",
    "        p = np.random.uniform(0.3, 0.7)\n",
    "        layer = Dropout(p)\n",
    "        layer.train()\n",
    "\n",
    "        layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "        next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "        # 1. check layer output\n",
    "        layer_output = layer.updateOutput(layer_input)\n",
    "        assertTrue(np.all(np.logical_or(np.isclose(layer_output, 0), \n",
    "                                    np.isclose(layer_output*(1.-p), layer_input))))\n",
    "\n",
    "        # 2. check layer input grad\n",
    "        layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n",
    "        assertTrue(np.all(np.logical_or(np.isclose(layer_grad, 0), \n",
    "                                    np.isclose(layer_grad*(1.-p), next_layer_grad))))\n",
    "\n",
    "        # 3. check evaluation mode\n",
    "        layer.evaluate()\n",
    "        layer_output = layer.updateOutput(layer_input)\n",
    "        assertAlmostEqual(layer_output, layer_input)\n",
    "\n",
    "        # 4. check mask\n",
    "        p = 0.0\n",
    "        layer = Dropout(p)\n",
    "        layer.train()\n",
    "        layer_output = layer.updateOutput(layer_input)\n",
    "        assertAlmostEqual(layer_output, layer_input)\n",
    "\n",
    "        p = 0.5\n",
    "        layer = Dropout(p)\n",
    "        layer.train()\n",
    "        layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "        next_layer_grad = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "        layer_output = layer.updateOutput(layer_input)\n",
    "        zeroed_elem_mask = np.isclose(layer_output, 0)\n",
    "        layer_grad = layer.updateGradInput(layer_input, next_layer_grad)        \n",
    "        assertTrue(np.all(zeroed_elem_mask == np.isclose(layer_grad, 0)))\n",
    "\n",
    "        # 5. dropout mask should be generated independently for every input matrix element, not for row/column\n",
    "        batch_size, n_in = 1000, 1\n",
    "        p = 0.8\n",
    "        layer = Dropout(p)\n",
    "        layer.train()\n",
    "\n",
    "        layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "        layer_output = layer.updateOutput(layer_input)\n",
    "        assertTrue(np.sum(np.isclose(layer_output, 0)) != layer_input.size)\n",
    "\n",
    "        layer_input = layer_input.T\n",
    "        layer_output = layer.updateOutput(layer_input)\n",
    "        assertTrue(np.sum(np.isclose(layer_output, 0)) != layer_input.size)\n",
    "        \n",
    "# test_Dropout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super(ReLU, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.maximum(input, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ReLU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Leaky ReLU\n",
    "Implement [**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs). Expriment with slope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU(Module):\n",
    "    def __init__(self, slope = 0.03):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "            \n",
    "        self.slope = slope\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        # Your code goes here. ################################################\n",
    "        output = input\n",
    "        if (input <= 0):\n",
    "            output = np.multiply(self.slope, output)\n",
    "        return  self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # Your code goes here. ################################################\n",
    "        relu_grad = self.slope\n",
    "        if (input > 0):\n",
    "            relu_grad = 1\n",
    "        self.gradInput = np.multiply(grad_output,relu_grad) \n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"LeakyReLU\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LeakyReLU():\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    batch_size, n_in = 2, 4\n",
    "    for _ in range(100):\n",
    "        # layers initialization\n",
    "        slope = np.random.uniform(0.01, 0.05)\n",
    "        torch_layer = torch.nn.LeakyReLU(slope)\n",
    "        custom_layer = LeakyReLU(slope)\n",
    "\n",
    "        layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "        next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "        # 1. check layer output\n",
    "        custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "        torch_layer_output_var = torch_layer(layer_input_var)\n",
    "        assertAlmostEqual(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6)\n",
    "\n",
    "        # 2. check layer input grad\n",
    "        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "        torch_layer_grad_var = layer_input_var.grad\n",
    "        assertAlmostEqual(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6)\n",
    "        \n",
    "# test_LeakyReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ELU\n",
    "Implement [**Exponential Linear Units**](http://arxiv.org/abs/1511.07289) activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELU(Module):\n",
    "    def __init__(self, alpha = 1.0):\n",
    "        super(ELU, self).__init__()\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        self.output = input\n",
    "        if (input <= 0):\n",
    "            self.output = np.multiply(self.alpha, np.expm1(input))\n",
    "        # Your code goes here. ################################################\n",
    "        return  self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        relu_grad = 1\n",
    "        if (input <= 0):\n",
    "            relu_grad = np.multiply(self.alpha, np.expm1(input))\n",
    "        self.gradInput = np.multiply(grad_output,relu_grad) \n",
    "        # Your code goes here. ################################################\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ELU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ELU():\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    batch_size, n_in = 2, 4\n",
    "    for _ in range(100):\n",
    "        # layers initialization\n",
    "        alpha = 1.0\n",
    "        torch_layer = torch.nn.ELU(alpha)\n",
    "        custom_layer = ELU(alpha)\n",
    "\n",
    "        layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "        next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "        # 1. check layer output\n",
    "        custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "        torch_layer_output_var = torch_layer(layer_input_var)\n",
    "        assertAlmostEqual(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6)\n",
    "\n",
    "        # 2. check layer input grad\n",
    "        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "        torch_layer_grad_var = layer_input_var.grad\n",
    "        assertAlmostEqual(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6)\n",
    "        \n",
    "# test_ELU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SoftPlus\n",
    "Implement [**SoftPlus**](https://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29) activations. Look, how they look a lot like ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftPlus(Module):\n",
    "    def __init__(self):\n",
    "        super(SoftPlus, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        # ln(1 + e^x)\n",
    "        self.output = np.log1p(numpy.exp(input))   \n",
    "        return  self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        #1 / 1 + e^-x\n",
    "        relu_grad = np.divide(1, 1 + np.exp(-input))\n",
    "        self.gradInput = gradOutput * relu_grad \n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SoftPlus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_SoftPlus():\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    batch_size, n_in = 2, 4\n",
    "    for _ in range(100):\n",
    "        # layers initialization\n",
    "        torch_layer = torch.nn.Softplus()\n",
    "        custom_layer = SoftPlus()\n",
    "\n",
    "        layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "        next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "        # 1. check layer output\n",
    "        custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "        torch_layer_output_var = torch_layer(layer_input_var)\n",
    "        assertAlmostEqual(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6)\n",
    "\n",
    "        # 2. check layer input grad\n",
    "        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "        torch_layer_grad_var = layer_input_var.grad\n",
    "        assertAlmostEqual(np.allclose(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "        \n",
    "# test_SoftPlus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criterions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criterions are used to score the models answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Criterion(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the loss function \n",
    "            associated to the criterion and return the result.\n",
    "            \n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateOutput`.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input, target)\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the gradients of the loss function\n",
    "            associated to the criterion and return the result. \n",
    "\n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateGradInput`.\n",
    "        \"\"\"\n",
    "        return self.updateGradInput(input, target)\n",
    "    \n",
    "    def updateOutput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.gradInput   \n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want \n",
    "        to have readable description. \n",
    "        \"\"\"\n",
    "        return \"Criterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **MSECriterion**, which is basic L2 norm usually used for regression, is implemented here for you.\n",
    "- input:   **`batch_size x n_feats`**\n",
    "- target: **`batch_size x n_feats`**\n",
    "- output: **scalar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSECriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        super(MSECriterion, self).__init__()\n",
    "        \n",
    "    def updateOutput(self, input, target):   \n",
    "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
    "        return self.output \n",
    " \n",
    "    def updateGradInput(self, input, target):\n",
    "        self.gradInput  = (input - target) * 2 / input.shape[0]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MSECriterion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Negative LogLikelihood criterion (numerically unstable)\n",
    "You task is to implement the **ClassNLLCriterion**. It should implement [multiclass log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss). Nevertheless there is a sum over `y` (target) in that formula, \n",
    "remember that targets are one-hot encoded. This fact simplifies the computations a lot. Note, that criterions are the only places, where you divide by batch size. Also there is a small hack with adding small number to probabilities to avoid computing log(0).\n",
    "- input:   **`batch_size x n_feats`** - probabilities\n",
    "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
    "- output: **scalar**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassNLLCriterionUnstable(Criterion):\n",
    "    EPS = 1e-15\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterionUnstable, self)\n",
    "        super(ClassNLLCriterionUnstable, self).__init__()\n",
    "        \n",
    "    def updateOutput(self, input, target): \n",
    "        \n",
    "        # Use this trick to avoid numerical errors\n",
    "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
    "        \n",
    "        # Your code goes here. ################################################\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        \n",
    "        # Use this trick to avoid numerical errors\n",
    "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
    "                \n",
    "        # Your code goes here. ################################################\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterionUnstable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ClassNLLCriterionUnstable():\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    batch_size, n_in = 2, 4\n",
    "    for i in range(100):\n",
    "        print(f\"Iter {i}\")\n",
    "        # layers initialization\n",
    "        torch_layer = torch.nn.NLLLoss()\n",
    "        custom_layer = ClassNLLCriterionUnstable()\n",
    "\n",
    "        layer_input = np.random.uniform(0, 1, (batch_size, n_in)).astype(np.float32)\n",
    "        layer_input /= layer_input.sum(axis=-1, keepdims=True)\n",
    "        layer_input = layer_input.clip(custom_layer.EPS, 1. - custom_layer.EPS)  # unifies input\n",
    "        target_labels = np.random.choice(n_in, batch_size)\n",
    "        target = np.zeros((batch_size, n_in), np.float32)\n",
    "        target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n",
    "\n",
    "        # 1. check layer output\n",
    "        custom_layer_output = custom_layer.updateOutput(layer_input, target)\n",
    "        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "        torch_layer_output_var = torch_layer(torch.log(layer_input_var), \n",
    "                                             Variable(torch.from_numpy(target_labels), requires_grad=False))\n",
    "        assertAlmostEqual(torch_layer_output_var.data.numpy(), custom_layer_output, msg=\"1. check layer output\")\n",
    "\n",
    "        # 2. check layer input grad\n",
    "        custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n",
    "        torch_layer_output_var.backward()\n",
    "        torch_layer_grad_var = layer_input_var.grad\n",
    "        assertAlmostEqual(torch_layer_grad_var.data.numpy(), custom_layer_grad, msg='2. check layer input grad')\n",
    "        \n",
    "test_ClassNLLCriterionUnstable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Negative LogLikelihood criterion (numerically stable)\n",
    "- input:   **`batch_size x n_feats`** - log probabilities\n",
    "- target: **`batch_size x n_feats`** - one-hot representation of ground truth\n",
    "- output: **scalar**\n",
    "\n",
    "Task is similar to the previous one, but now the criterion input is the output of log-softmax layer. This decomposition allows us to avoid problems with computation of forward and backward of log()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassNLLCriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterion, self)\n",
    "        super(ClassNLLCriterion, self).__init__()\n",
    "        \n",
    "    def updateOutput(self, input, target): \n",
    "        # Your code goes here. ################################################\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        # Your code goes here. ################################################\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ClassNLLCriterion():\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    batch_size, n_in = 2, 4\n",
    "    for i in range(100):\n",
    "        print(f\"test_ClassNLLCriterion. Iter {i}\")\n",
    "        # layers initialization\n",
    "        torch_layer = torch.nn.NLLLoss()\n",
    "        custom_layer = ClassNLLCriterion()\n",
    "\n",
    "        layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "        layer_input = torch.nn.LogSoftmax(dim=1)(Variable(torch.from_numpy(layer_input))).data.numpy()\n",
    "        target_labels = np.random.choice(n_in, batch_size)\n",
    "        target = np.zeros((batch_size, n_in), np.float32)\n",
    "        target[np.arange(batch_size), target_labels] = 1  # one-hot encoding\n",
    "\n",
    "        # 1. check layer output\n",
    "        custom_layer_output = custom_layer.updateOutput(layer_input, target)\n",
    "        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "        torch_layer_output_var = torch_layer(layer_input_var, \n",
    "                                             Variable(torch.from_numpy(target_labels), requires_grad=False))\n",
    "        assertAlmostEqual(torch_layer_output_var.data.numpy(), custom_layer_output, msg='1. check layer output')\n",
    "\n",
    "        # 2. check layer input grad\n",
    "        custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n",
    "        torch_layer_output_var.backward()\n",
    "        torch_layer_grad_var = layer_input_var.grad\n",
    "        assertAlmostEqual(torch_layer_grad_var.data.numpy(), custom_layer_grad, msg='2. check layer input grad')\n",
    "        \n",
    "test_ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD optimizer with momentum\n",
    "- `variables` - list of lists of variables (one list per layer)\n",
    "- `gradients` - list of lists of current gradients (same structure as for `variables`, one array for each var)\n",
    "- `config` - dict with optimization parameters (`learning_rate` and `momentum`)\n",
    "- `state` - dict with optimizator state (used to save accumulated gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_momentum(variables, gradients, config, state):  \n",
    "    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
    "    state.setdefault('accumulated_grads', {})\n",
    "    \n",
    "    var_index = 0 \n",
    "    for current_layer_vars, current_layer_grads in zip(variables, gradients): \n",
    "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
    "            \n",
    "            old_grad = state['accumulated_grads'].setdefault(var_index, np.zeros_like(current_grad))\n",
    "            \n",
    "            np.add(config['momentum'] * old_grad, config['learning_rate'] * current_grad, out=old_grad)\n",
    "            \n",
    "            current_var -= old_grad\n",
    "            var_index += 1     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. [Adam](https://arxiv.org/pdf/1412.6980.pdf) optimizer\n",
    "- `variables` - list of lists of variables (one list per layer)\n",
    "- `gradients` - list of lists of current gradients (same structure as for `variables`, one array for each var)\n",
    "- `config` - dict with optimization parameters (`learning_rate`, `beta1`, `beta2`, `epsilon`)\n",
    "- `state` - dict with optimizator state (used to save 1st and 2nd moment for vars)\n",
    "\n",
    "Formulas for optimizer:\n",
    "\n",
    "Current step learning rate: $$\\text{lr}_t = \\text{learning_rate} * \\frac{\\sqrt{1-\\beta_2^t}} {1-\\beta_1^t}$$\n",
    "First moment of var: $$\\mu_t = \\beta_1 * \\mu_{t-1} + (1 - \\beta_1)*g$$ \n",
    "Second moment of var: $$v_t = \\beta_2 * v_{t-1} + (1 - \\beta_2)*g*g$$\n",
    "New values of var: $$\\text{variable} = \\text{variable} - \\text{lr}_t * \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_optimizer(variables, gradients, config, state):  \n",
    "    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
    "    state.setdefault('m', {})  # first moment vars\n",
    "    state.setdefault('v', {})  # second moment vars\n",
    "    state.setdefault('t', 0)   # timestamp\n",
    "    state['t'] += 1\n",
    "    for k in ['learning_rate', 'beta1', 'beta2', 'epsilon']:\n",
    "        assert k in config, config.keys()\n",
    "    \n",
    "    var_index = 0 \n",
    "    lr_t = config['learning_rate'] * np.sqrt(1 - config['beta2']**state['t']) / (1 - config['beta1']**state['t'])\n",
    "    for current_layer_vars, current_layer_grads in zip(variables, gradients): \n",
    "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
    "            var_first_moment = state['m'].setdefault(var_index, np.zeros_like(current_grad))\n",
    "            var_second_moment = state['v'].setdefault(var_index, np.zeros_like(current_grad))\n",
    "            \n",
    "            # <YOUR CODE> #######################################\n",
    "            # update `current_var_first_moment`, `var_second_moment` and `current_var` values\n",
    "            #np.add(... , out=var_first_moment)\n",
    "            #np.add(... , out=var_second_moment)\n",
    "            #current_var -= ...\n",
    "            \n",
    "            # small checks that you've updated the state; use np.add for rewriting np.arrays values\n",
    "            assert var_first_moment is state['m'].get(var_index)\n",
    "            assert var_second_moment is state['v'].get(var_index)\n",
    "            var_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_adam_optimizer():\n",
    "    state = {}  \n",
    "    config = {'learning_rate': 1e-3, 'beta1': 0.9, 'beta2':0.999, 'epsilon':1e-8}\n",
    "    variables = [[np.arange(10).astype(np.float64)]]\n",
    "    gradients = [[np.arange(10).astype(np.float64)]]\n",
    "    adam_optimizer(variables, gradients, config, state)\n",
    "    \n",
    "    assertAlmostEqual(state['m'][0], np.array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]))\n",
    "    assertAlmostEqual(state['v'][0], np.array([0., 0.001, 0.004, 0.009, 0.016, 0.025, 0.036, 0.049, 0.064, 0.081]))\n",
    "    assertTrue(state['t'] == 1)\n",
    "    assertAlmostEqual(variables[0][0], np.array([0., 0.999, 1.999, 2.999, 3.999, 4.999, 5.999, 6.999, 7.999, 8.999]))\n",
    "    adam_optimizer(variables, gradients, config, state)\n",
    "    assertAlmostEqual(state['m'][0], np.array([0., 0.19, 0.38, 0.57, 0.76, 0.95, 1.14, 1.33, 1.52, 1.71]))\n",
    "    assertAlmostEqual(state['v'][0], np.array([0., 0.001999, 0.007996, 0.017991, 0.031984, 0.049975, 0.071964, 0.097951, 0.127936, 0.161919]))\n",
    "    assertTrue(state['t'] == 2)\n",
    "    assertAlmostEqual(variables[0][0], np.array([0., 0.998, 1.998, 2.998, 3.998, 4.998, 5.998, 6.998, 7.998, 8.998]))\n",
    "    \n",
    "# test_adam_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers for advanced track homework\n",
    "You **don't need** to implement it if you are working on `homework_main-basic.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conv2d [Advanced]\n",
    "- input:   **`batch_size x in_channels x h x w`**\n",
    "- output: **`batch_size x out_channels x h x w`**\n",
    "\n",
    "You should implement something like pytorch `Conv2d` layer with `stride=1` and zero-padding outside of image using `scipy.signal.correlate` function.\n",
    "\n",
    "Practical notes:\n",
    "- While the layer name is \"convolution\", the most of neural network frameworks (including tensorflow and pytorch) implement operation that is called [correlation](https://en.wikipedia.org/wiki/Cross-correlation#Cross-correlation_of_deterministic_signals) in signal processing theory. So **don't use** `scipy.signal.convolve` since it implements [convolution](https://en.wikipedia.org/wiki/Convolution#Discrete_convolution) in terms of signal processing.\n",
    "- It may be convenient to use `skimage.util.pad` for zero-padding.\n",
    "- It's rather ok to implement convolution over 4d array using 2 nested loops: one over batch size dimension and another one over output filters dimension\n",
    "- Having troubles with understanding how to implement the layer? \n",
    " - Check the last year video of lecture 3 (starting from ~1:14:20)\n",
    " - May the google be with you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import skimage\n",
    "\n",
    "class Conv2d(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(Conv2d, self).__init__()\n",
    "        assert kernel_size % 2 == 1, kernel_size\n",
    "       \n",
    "        stdv = 1./np.sqrt(in_channels)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size = (out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size=(out_channels,))\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        pad_size = self.kernel_size // 2\n",
    "        # YOUR CODE ##############################\n",
    "        # 1. zero-pad the input array\n",
    "        # 2. compute convolution using scipy.signal.correlate(... , mode='valid')\n",
    "        # 3. add bias value\n",
    "        \n",
    "        # self.output = ...\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        pad_size = self.kernel_size // 2\n",
    "        # YOUR CODE ##############################\n",
    "        # 1. zero-pad the gradOutput\n",
    "        # 2. compute 'self.gradInput' value using scipy.signal.correlate(... , mode='valid')\n",
    "        \n",
    "        # self.gradInput = ...\n",
    "        \n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        pad_size = self.kernel_size // 2\n",
    "        # YOUR CODE #############\n",
    "        # 1. zero-pad the input\n",
    "        # 2. compute 'self.gradW' using scipy.signal.correlate(... , mode='valid')\n",
    "        # 3. compute 'self.gradb' - formulas like in Linear of ChannelwiseScaling layers\n",
    "        \n",
    "        # self.gradW = ...\n",
    "        # self.gradb = ...\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Conv2d %d -> %d' %(s[1],s[0])\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Conv2d(self):\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    batch_size, n_in, n_out = 2, 3, 4\n",
    "    h,w = 5,6\n",
    "    kern_size = 3\n",
    "    for _ in range(100):\n",
    "        # layers initialization\n",
    "        torch_layer = torch.nn.Conv2d(n_in, n_out, kern_size, padding=1)\n",
    "        custom_layer = Conv2d(n_in, n_out, kern_size)\n",
    "        custom_layer.W = torch_layer.weight.data.numpy() # [n_out, n_in, kern, kern]\n",
    "        custom_layer.b = torch_layer.bias.data.numpy()\n",
    "\n",
    "        layer_input = np.random.uniform(-1, 1, (batch_size, n_in, h,w)).astype(np.float32)\n",
    "        next_layer_grad = np.random.uniform(-1, 1, (batch_size, n_out, h, w)).astype(np.float32)\n",
    "\n",
    "        # 1. check layer output\n",
    "        custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "        torch_layer_output_var = torch_layer(layer_input_var)\n",
    "        assertAlmostEqual(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6)\n",
    "\n",
    "        # 2. check layer input grad\n",
    "        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "        torch_layer_grad_var = layer_input_var.grad\n",
    "        assertAlmostEqual(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6))\n",
    "\n",
    "        # 3. check layer parameters grad\n",
    "        custom_layer.accGradParameters(layer_input, next_layer_grad)\n",
    "        weight_grad = custom_layer.gradW\n",
    "        bias_grad = custom_layer.gradb\n",
    "        torch_weight_grad = torch_layer.weight.grad.data.numpy()\n",
    "        torch_bias_grad = torch_layer.bias.grad.data.numpy()\n",
    "        #m = ~np.isclose(torch_weight_grad, weight_grad, atol=1e-5)\n",
    "        assertAlmostEqual(torch_weight_grad, weight_grad, atol=1e-6, )\n",
    "        assertAlmostEqual(torch_bias_grad, bias_grad, atol=1e-6)\n",
    "        \n",
    "# test_Conv2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. MaxPool2d [Advanced]\n",
    "- input:   **`batch_size x n_input_channels x h x w`**\n",
    "- output: **`batch_size x n_output_channels x h // kern_size x w // kern_size`**\n",
    "\n",
    "You are to implement simplified version of pytorch `MaxPool2d` layer with stride = kernel_size. Please note, that it's not a common case that stride = kernel_size: in AlexNet and ResNet kernel_size for max-pooling was set to 3, while stride was set to 2. We introduce this restriction to make implementation simplier.\n",
    "\n",
    "Practical notes:\n",
    "- During forward pass what you need to do is just to reshape the input tensor to `[n, c, h / kern_size, kern_size, w / kern_size, kern_size]`, swap two axes and take maximums over the last two dimensions. Reshape + axes swap is sometimes called space-to-batch transform.\n",
    "- During backward pass you need to place the gradients in positions of maximal values taken during the forward pass\n",
    "- In real frameworks the indices of maximums are stored in memory during the forward pass. It is cheaper than to keep the layer input in memory and recompute the maximums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2d(Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(MaxPool2d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.gradInput = None\n",
    "                    \n",
    "    def updateOutput(self, input):\n",
    "        input_h, input_w = input.shape[-2:]\n",
    "        # your may remove these asserts and implement MaxPool2d with padding\n",
    "        assert input_h % self.kernel_size == 0  \n",
    "        assert input_w % self.kernel_size == 0\n",
    "        \n",
    "        # YOUR CODE #############################\n",
    "        # self.output = ...\n",
    "        # self.max_indices = ...\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        # YOUR CODE #############################\n",
    "        # self.gradInput = ...\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        q = 'MaxPool2d, kern %d, stride %d' %(self.kernel_size, self.kernel_size)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_MaxPool2d():\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    batch_size, n_in = 2, 3\n",
    "    h,w = 4,6\n",
    "    kern_size = 2\n",
    "    for _ in range(100):\n",
    "        # layers initialization\n",
    "        torch_layer = torch.nn.MaxPool2d(kern_size)\n",
    "        custom_layer = MaxPool2d(kern_size)\n",
    "\n",
    "        layer_input = np.random.uniform(-10, 10, (batch_size, n_in, h,w)).astype(np.float32)\n",
    "        next_layer_grad = np.random.uniform(-10, 10, (batch_size, n_in, \n",
    "                                                      h // kern_size, w // kern_size)).astype(np.float32)\n",
    "\n",
    "        # 1. check layer output\n",
    "        custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "        layer_input_var = Variable(torch.from_numpy(layer_input), requires_grad=True)\n",
    "        torch_layer_output_var = torch_layer(layer_input_var)\n",
    "        assertAlmostEqual(torch_layer_output_var.data.numpy(), custom_layer_output, atol=1e-6)\n",
    "\n",
    "        # 2. check layer input grad\n",
    "        custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "        torch_layer_output_var.backward(torch.from_numpy(next_layer_grad))\n",
    "        torch_layer_grad_var = layer_input_var.grad\n",
    "        assertAlmostEqual(torch_layer_grad_var.data.numpy(), custom_layer_grad, atol=1e-6)\n",
    "        \n",
    "test_MaxPool2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten layer\n",
    "Just reshapes inputs and gradients. It's usually used as proxy layer between Conv2d and Linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Module):\n",
    "    def __init__(self):\n",
    "         super(Flatten, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = input.reshape(len(input), -1)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = gradOutput.reshape(input.shape)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Flatten\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
